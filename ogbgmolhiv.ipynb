{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3432edc5",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we will build a model to predict if a molecule inhibits HIV replication using the [ogbg-molhiv](https://ogb.stanford.edu/docs/graphprop/#ogbg-mol) dataset.  This is derived from the [MoleculeNet](https://moleculenet.org/) dataset and represents each molecule as a graph, where the nodes are atoms and the edges are chemical bonds.  We will treat this as a graph classification task and use a modified version of the [Graph Isomorphism Network](https://arxiv.org/abs/1810.00826) (GIN). Once we create this benchmark, we will add the idea of a [Virtual Node](https://arxiv.org/abs/1704.01212). \n",
    "\n",
    "## What you need to do\n",
    "### Basics\n",
    "\n",
    "In this task we're going to apply a GNN to the molecular structure to get node embeddings for each atom and then apply a \"Readout\" function to combine those node embeddings into a representation of the graph.  Instead of having batches of nodes like previous exercises, this time we'll be dealing with batches of small graphs.  This difference will result in some additional implementation complexity.  Sections you need to write code for will be marked with a \"TODO\" comment. For ease of troubleshooting, do not modify any of the code except where indicated until after the basics have been achieved.  We will also import functionality from the `utils.py` script.  No changes should be necessary to this file.\n",
    "\n",
    "### Improvements\n",
    "\n",
    "Once you have a correct implementation and can match leaderboard performance, we can move on to experimentation. In this section, we will implement \"Virtual Nodes\", which are nodes added to each graph that connect to all other nodes and therefore provide a short communication path between all nodes in a graph.\n",
    "\n",
    "### Statistics\n",
    "\n",
    "In each section, the `repeat_experiments` function is used to run the same model building process multiple times to collect statistics since there is significant variation in performance from run-to-run.  This allows us to make apples-to-apples comparisons to the leaderboard, but this process takes a long time.  Feel free to reduce from the default setup of 10 experiments with 100 epochs each to something much smaller (e.g., 1 experiment with 5 epochs) until you're confident in the implementation.\n",
    "\n",
    "\n",
    "### Extra Credit\n",
    "\n",
    "A great way to learn is to 1) read other peoples' code and 2) tinker. If you want to maximize your learning, the Extra Credit section will provide a few directions for further exploration. As part of this, it's recommended you find another submission on the OGB Leaderboard and try to re-create its submission. This will likely involve reading the paper and code, which is great practice. If you do something interesting and novel, we'll kindly host it in a public repo as an example of your glory (if you wish).\n",
    "\n",
    "\n",
    "# Boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e1a564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import math\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2164ab17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 1.7.1.  CUDA version: 10.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Torch version: {th.__version__}.  CUDA version: {th.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "550e6698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://data.dgl.ai/wheels/repo.html\n",
      "Requirement already satisfied: dgl-cu101 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (0.7.2)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from dgl-cu101) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from dgl-cu101) (2.26.0)\n",
      "Requirement already satisfied: networkx>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from dgl-cu101) (2.5)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from dgl-cu101) (1.19.5)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from networkx>=2.1->dgl-cu101) (4.4.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests>=2.19.0->dgl-cu101) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests>=2.19.0->dgl-cu101) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests>=2.19.0->dgl-cu101) (2.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests>=2.19.0->dgl-cu101) (3.3)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_latest_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Follow instructions at https://www.dgl.ai/pages/start.html\n",
    "!{sys.executable} -m pip install dgl-cu101 -f https://data.dgl.ai/wheels/repo.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2343909",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ogb in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (1.3.2)\n",
      "Requirement already satisfied: urllib3>=1.24.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from ogb) (1.26.7)\n",
      "Requirement already satisfied: pandas>=0.24.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from ogb) (1.1.5)\n",
      "Requirement already satisfied: tqdm>=4.29.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from ogb) (4.62.3)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from ogb) (1.19.5)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from ogb) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from ogb) (0.24.1)\n",
      "Requirement already satisfied: outdated>=0.2.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from ogb) (0.2.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from ogb) (1.7.1)\n",
      "Requirement already satisfied: littleutils in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from outdated>=0.2.0->ogb) (0.2.2)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from outdated>=0.2.0->ogb) (2.26.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from pandas>=0.24.0->ogb) (2021.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from scikit-learn>=0.20.0->ogb) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from scikit-learn>=0.20.0->ogb) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from scikit-learn>=0.20.0->ogb) (1.5.3)\n",
      "Requirement already satisfied: typing_extensions in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from torch>=1.6.0->ogb) (3.10.0.2)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from torch>=1.6.0->ogb) (0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->outdated>=0.2.0->ogb) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->outdated>=0.2.0->ogb) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->outdated>=0.2.0->ogb) (2.0.7)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_latest_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install ogb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2a863e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "os.environ['DGLBACKEND'] = 'pytorch'\n",
    "import dgl\n",
    "from dgl.data import DGLDataset\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import dgl.function as fn\n",
    "\n",
    "from ogb.graphproppred import DglGraphPropPredDataset, Evaluator\n",
    "from ogb.graphproppred.mol_encoder import BondEncoder, AtomEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c330ebec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.7.2'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgl.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad28c8f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if th.cuda.is_available() else 'cpu'\n",
    "device = th.device(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f83681b",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eacab763",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2854dd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import repeat_experiments, norm_plot, train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369035e8",
   "metadata": {},
   "source": [
    "## Capture GIN leaderboard performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "6230c5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values from the OGB leaderboard of GIN, submitted May 01, 2020\n",
    "val_auc_lb, val_auc_lb_std, test_auc_lb, test_auc_lb_std = 0.8232, 0.0090, 0.7558, 0.0140\n",
    "evaluator = Evaluator(name = \"ogbg-molhiv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3208f85d",
   "metadata": {},
   "source": [
    "# Data \n",
    "\n",
    "This dataset consists of 41k molecule graphs with an average of 25.5 nodes and 27.5 edges per graph.  The nodes and edges both have natural features like atomic number, bond type...etc (full list [here](https://github.com/snap-stanford/ogb/blob/68a303f320220cda859e83e3a8660f2b9debedf6/ogb/utils/features.py#L2-L35)).  The binary target variable encodes whether or not the molecule inhibits HIV replication and we therefore treat this as a binary classification task and evaluate with ROC AUC.  The graphs are split using a...\n",
    "\n",
    "> _scaffold splitting_ procedure that splits the molecules based on their two-dimensional structural frameworks. The scaffold splitting attempts to separate structurally different molecules into different subsets, which provides a more realistic estimate of model performance in prospective experimental settings\n",
    "\n",
    "More details on the dataset can be found on the [OGB dataset page](https://ogb.stanford.edu/docs/graphprop/#ogbg-mol).\n",
    "\n",
    "## Create DGL dataset using OGB\n",
    "We will use the `ogb` python package to download the data, give us the correct train/validation/test splits for proper comparison and an `Evaluator` to make sure performance is measured appropriately.  To properly handle the batching of graphs with their labels, we create a wrapper around the OGB dataset, according to process explained [here](https://docs.dgl.ai/tutorials/blitz/6_load_data.html#creating-a-dataset-for-graph-classification-from-csv).  In essence, we need an object that returns both the graph and the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "036a79af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolHIVDataset(DGLDataset):\n",
    "    \"\"\"\n",
    "    A wrapper around the OGB dataset so that the label is returned along \n",
    "    with the graph, as specified in __getitem__.\n",
    "    See:  https://docs.dgl.ai/tutorials/blitz/6_load_data.html#creating-a-dataset-for-graph-classification-from-csv\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__(name='ogbg-molhiv')\n",
    "        \n",
    "    def _load(self):\n",
    "        self.dataset = DglGraphPropPredDataset(name=\"ogbg-molhiv\", root = 'dataset/')\n",
    "        self.split_idx = self.dataset.get_idx_split()\n",
    "        print(self.dataset.meta_info)\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        return self.dataset.graphs[i], self.dataset.labels[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset.graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "469af45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num tasks                                                                1\n",
      "eval metric                                                         rocauc\n",
      "download_name                                                          hiv\n",
      "version                                                                  1\n",
      "url                      http://snap.stanford.edu/ogb/data/graphproppre...\n",
      "add_inverse_edge                                                      True\n",
      "data type                                                              mol\n",
      "has_node_attr                                                         True\n",
      "has_edge_attr                                                         True\n",
      "task type                                            binary classification\n",
      "num classes                                                              2\n",
      "split                                                             scaffold\n",
      "additional node files                                                 None\n",
      "additional edge files                                                 None\n",
      "binary                                                               False\n",
      "Name: ogbg-molhiv, dtype: object\n"
     ]
    }
   ],
   "source": [
    "dataset = MolHIVDataset()\n",
    "assert len(dataset) == 41127"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73673b0d",
   "metadata": {},
   "source": [
    "### Data loading with batches of graphs\n",
    "\n",
    "For efficiency reasons, the graphs in a minibatch are combined into one \"super-graph\" in which each component graph is disconnected from the others.  This enables us to do message passing across all the graphs in the minibatch simultaneously, but comes with the added complexity of needing to keep track of which nodes belong to which graph.  DGL has built-in tools for dealing with this complexity and we'll make use of them throughout this exercise.  Please review the dataloader code in the following cells if you want to see how this works.  Just keep in mind two things: 1) the graph we're passing into the models is the \"super-graph\" which has all the component graphs in the minibatch combined into one data structure and 2) we'll have to unwind this process when converting from node-level to graph-level representations.\n",
    "\n",
    "### Create train/val/test dataloaders\n",
    "See: https://docs.dgl.ai/tutorials/blitz/5_graph_classification.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89873023",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_idx = dataset.split_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a53dde20",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = SubsetRandomSampler(split_idx['train'])\n",
    "val_subset = Subset(dataset, split_idx['valid'])\n",
    "test_subset = Subset(dataset, split_idx['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "512a385d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 32\n",
    "train_dataloader = GraphDataLoader(\n",
    "    dataset, sampler=train_sampler, batch_size=bs, drop_last=False)\n",
    "val_dataloader = GraphDataLoader(\n",
    "    val_subset, shuffle=False, batch_size=bs, drop_last=False)\n",
    "test_dataloader = GraphDataLoader(\n",
    "    test_subset, shuffle=False, batch_size=bs, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0166d3b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4113, 4113)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_subset), len(test_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "f3374da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(train_dataloader)\n",
    "batch = next(it)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fb1ec3",
   "metadata": {},
   "source": [
    "The batch returned from the dataloader contains two elements: \n",
    "\n",
    "1. A single \"super-graph\" with all the graphs in the minibatch combined\n",
    "2. A tensor of labels for each graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "3e623a50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Graph(num_nodes=726, num_edges=1554,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " tensor([[0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [1],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0],\n",
       "         [0]])]"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e614d591",
   "metadata": {},
   "source": [
    "Calling `dgl.unbatch` on the single graph will break it into a list of its original component graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "6ccf0baa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Graph(num_nodes=25, num_edges=54,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=21, num_edges=46,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=27, num_edges=52,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=22, num_edges=48,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=22, num_edges=48,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=25, num_edges=56,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=38, num_edges=84,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=26, num_edges=58,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=31, num_edges=72,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=35, num_edges=76,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=12, num_edges=22,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=15, num_edges=32,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=21, num_edges=46,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=27, num_edges=58,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=11, num_edges=24,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=32, num_edges=70,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=35, num_edges=76,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=24, num_edges=50,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=15, num_edges=30,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=19, num_edges=42,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=33, num_edges=68,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=37, num_edges=80,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=23, num_edges=54,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=21, num_edges=42,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=10, num_edges=18,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=19, num_edges=42,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=17, num_edges=36,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=13, num_edges=26,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=23, num_edges=44,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=21, num_edges=48,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=10, num_edges=20,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)}),\n",
       " Graph(num_nodes=16, num_edges=32,\n",
       "       ndata_schemes={'feat': Scheme(shape=(9,), dtype=torch.int64)}\n",
       "       edata_schemes={'feat': Scheme(shape=(3,), dtype=torch.int64)})]"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgl.unbatch(batch[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dae6db2",
   "metadata": {},
   "source": [
    "### Node and Edge Features\n",
    "In this case the natural features for both nodes and edges are categorical in nature and the feature data has been encoded into a tensor of integers such that an integer in a column maps to some categorical value (e.g., `bond_type = ['single', 'double', 'triple'] -> [0, 1, 2]`).  These node and edge features are assigned to the graph structure and accessible via `g.ndata['feat']` and `g.edata['feat']`, respectively.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "b1918d4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7,  0,  1,  ...,  1,  0,  0],\n",
       "        [ 5,  0,  3,  ...,  1,  0,  0],\n",
       "        [ 5,  0,  4,  ...,  2,  0,  0],\n",
       "        ...,\n",
       "        [ 5,  0,  3,  ...,  1,  1,  1],\n",
       "        [ 5,  0,  3,  ...,  1,  1,  1],\n",
       "        [16,  0,  1,  ...,  2,  0,  0]])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].ndata['feat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "6a1e6a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 1],\n",
       "        [1, 0, 1],\n",
       "        [0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0],\n",
       "        [3, 0, 1],\n",
       "        [3, 0, 1]])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].edata['feat']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bc4d48",
   "metadata": {},
   "source": [
    "OGB provides two built-in classes for converting these integer encodings into float values via the use of lookup embedding layers:  [AtomEncoder](https://github.com/snap-stanford/ogb/blob/68a303f320220cda859e83e3a8660f2b9debedf6/ogb/graphproppred/mol_encoder.py#L7) and [BondEncoder](https://github.com/snap-stanford/ogb/blob/68a303f320220cda859e83e3a8660f2b9debedf6/ogb/graphproppred/mol_encoder.py#L27).  These classes essentially create an embedding lookup table *for each column* in the feature tensor, and then the integer value in a column serves as the row index in the lookup.  The size of each category embedding is specified with the `emb_dim` variable and therefore the number of learnable embedding parameters for a column in the feature tensor is `N_unique * emb_dim`, where $N_{unique}$ is the number of unique categories in the column.  \n",
    "\n",
    "Since a single node/edge has multiple categorical features, this process results in an embedding of size `emb_dim` for each categorical feature.  This is reduced to a single embedding of length `emb_dim` by summing these column embeddings together for a final representation of the atom or bond.  For example, if there are three categorical feature columns and we specify `emb_dim=5`, each feature will receive a 5-dim embedding, and then those three vectors will be summed to give a single 5-dim vector as a final representation.  This method is used for both the atom node features and the bond edge features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "790da1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a130ae8c",
   "metadata": {},
   "source": [
    "# Modeling \n",
    "\n",
    "In graph classification we typically use GNNs to calculate node embeddings and then combine those node embeddings in some way to get a representation of the graph.  This can be tricky when dealing with batches of graphs for training because we need to keep track of which nodes/edges belongs to which graph.  We therefore break our model into 3 main components: 1) the base convolution layer that implements the GIN update equation, 2) the node-level GNN that stacks GIN layers together to create node embeddings and 3) a graph-level GNN that combines node embeddings to create a graph embedding. \n",
    "\n",
    "## Graph Isomorphic Network layer\n",
    "\n",
    "First we will implement the GIN convolution layer.  The GIN model comes from the [How Powerful are Graph Neural Networks](https://arxiv.org/abs/1810.00826) paper.  The equation to update the embedding $\\mathbf{h}_i$ for node $i$ is given as:\n",
    "\n",
    "$$\\mathbf{h}_i^{(l+1)} = \\text{MLP}[ (1 + \\epsilon) \\cdot \\mathbf{h}_i^{(l)} + \\sum_{j \\in \\mathcal{N}_i} \\mathbf{h}_j^{(l)} ]$$\n",
    "\n",
    "where $\\epsilon$ is some learnable constant and $\\mathcal{N}_i$ indicates the 1-hop neighborhood of node $i$.  Effectively, it's the sum of the current embedding (adjusted by some scaling factor) added to the embeddings of all its neighbors, passed through an MLP.  \n",
    "\n",
    "We need to slightly modify this architecture to make use of the edge features, however.  We will use the method implemented on the Leaderboard ([code](https://github.com/snap-stanford/ogb/blob/master/examples/graphproppred/mol/conv.py#L31)), which tweaks the message function to add the edge features to the neighbor node embedding, and pass that sum through a ReLU activation.  In the original equation, the message function merely copies the neighbor embedding: $\\mathbf{m}_{ij} = \\mathbf{h}_j$.  Here, the message function will be modified to be: $\\mathbf{m}_{ij} = \\text{ReLU}(\\mathbf{h}_j + \\mathbf{e}_{ij})$.  The final update equation will therefore become:\n",
    "\n",
    "$$\\mathbf{h}_i^{(l+1)} = \\text{MLP}[ (1 + \\epsilon) \\cdot \\mathbf{h}_i^{(l)} + \\sum_{j \\in \\mathcal{N}_i} \\text{ReLU}(\\mathbf{h}_j^{(l)} + \\mathbf{e}_{ij}) ]$$\n",
    "\n",
    "This causes a slight implementation complication.  The easiest way to use DGL's message passing API is to pass in standard message and reduce functions to the `update_all` method.  For example, the message passing part of the original GIN equation ($\\sum_{j \\in \\mathcal{N}_i} \\mathbf{h}_j $) would be implemented like:  `g.update_all(fn.copy_u('x', 'm'), fn.sum('m', h')`.  Using these built-in message/reduce functions allow optimizations under the hood that improve memory consumption and computation speed.  However, there is no built-in functions that matches our modified message/reduce function $\\sum_{j \\in \\mathcal{N}_i} \\text{ReLU}(\\mathbf{h}_j + \\mathbf{e}_{ij})$.  \n",
    "\n",
    "We'll get around this by breaking up the computation into two steps.  Specifically, we can call `apply_edges` to create the messages (which become a new tensor defined on `g.edata`), but not yet aggregate them.  For example, calling `g.apply_edges(fn.copy_u('x', 'm'))` would copy the value of the `x` tensor for each node and assign it to each edge where that node is the source, and the result would be accessible via `g.edata['m']`.  In other words, this broadcasts node data to the edges.\n",
    "\n",
    "We can then modify that new edge tensor as an intermediate processing step (e.g., by calling `F.relu` on it), and finally call `update_all` using the `fn.copy_e` message function to use this new, modified message.  If we were implementing the original GIN equation with this two-step process without any modification, it would look something like the following:\n",
    "\n",
    "```python\n",
    "# Create messages along edges by copying source node data\n",
    "g.apply_edges(\n",
    "    fn.copy_u('x', 'm')\n",
    ")\n",
    "\n",
    "# Could modify g.edata['m'] here...\n",
    "\n",
    "\n",
    "# Aggregate messages on edges using \"sum\" to create new node tensor: \"mp\"\n",
    "g.update_all(\n",
    "    fn.copy_e('m', 'm'),\n",
    "    fn.sum('m', 'mp')\n",
    ")\n",
    "\n",
    "result = g.ndata['mp']\n",
    "\n",
    "```\n",
    "\n",
    "For this section, you'll find the DGL documentation on [built-in functions](https://docs.dgl.ai/api/python/dgl.function.html#dgl-built-in-function) useful, as you'll need to pick the right one for each sub-task.  You'll only be asked to modify the `forward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "c4aff2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GINLayer(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super(GINLayer, self).__init__()\n",
    "        \"\"\"\n",
    "        This class implements a single GIN layer with edge features as given by \n",
    "        the final equation above\n",
    "    \n",
    "        Arguments\n",
    "        ----------\n",
    "        emb_dim : int\n",
    "            Number of dimensions used for the embedding\n",
    "        \"\"\"\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 2*emb_dim), \n",
    "            nn.BatchNorm1d(2*emb_dim), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(2*emb_dim, emb_dim)\n",
    "        )\n",
    "        self.eps = nn.Parameter(th.FloatTensor([0]))\n",
    "        \n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        A method that defines how to re-initialize all the parameters of\n",
    "        the model\n",
    "        \"\"\"\n",
    "        # MLP\n",
    "        for layer in self.mlp:\n",
    "            if hasattr(layer, 'reset_parameters'):\n",
    "                layer.reset_parameters()\n",
    "        \n",
    "        # EPS\n",
    "        nn.init.constant_(self.eps, 0.)\n",
    "        \n",
    "    def extra_repr(self):\n",
    "        # Add \"eps\" to the string representation\n",
    "        return f'(eps): nn.Parameter'\n",
    "        \n",
    "    def forward(self, g, x_node, x_edge, h_virt=None):\n",
    "        \"\"\"\n",
    "        Applies the modified GIN update equation, which includes edge features\n",
    "        \n",
    "        Arguments\n",
    "        ----------\n",
    "        g : DGLGraph\n",
    "            The graph used for message passing\n",
    "        x_node : Tensor\n",
    "            The node features (i.e., AtomEncodings)\n",
    "        x_edge : Tensor\n",
    "            The edge features (i.e., BondEncodings)\n",
    "        h_virt : Tensor (optional)\n",
    "            The virtual node embedding\n",
    "        \"\"\"\n",
    "        \n",
    "        with g.local_scope():\n",
    "            # Store edge features to 'bond' key in g.edata\n",
    "            g.edata['bond'] = x_edge\n",
    "            # Store node features to 'x' key in g.ndata\n",
    "            g.srcdata['x'] = x_node\n",
    "            # TODO\n",
    "            #################################\n",
    "            \"\"\"\n",
    "            Break down the message passing to ultimately implement:\n",
    "                h_i = sum_j( F.relu (h_j + e_ij) )\n",
    "                \n",
    "            You'll want to reference the DGL built-in functions:\n",
    "            https://docs.dgl.ai/api/python/dgl.function.html#dgl-built-in-function\n",
    "                \n",
    "            1. use g.apply_edges to calculate 'x' + 'bond' to get \n",
    "                the message along each edge.  Use 'm' as the \"out\" value, \n",
    "                which will make the output accessible with g.edata['m']\n",
    "            \n",
    "            2. modify this tensor by applying the ReLU activation, \n",
    "                and storing back into the graph's edata\n",
    "                \n",
    "            3. sum these messages up using \"update_all\"\n",
    "\n",
    "                \n",
    "            \"\"\"\n",
    "            ## 1. take the sum\n",
    "            g.apply_edges(\n",
    "                # TODO\n",
    "                None\n",
    "            )\n",
    "            ## TODO: 2. apply ReLU\n",
    "            g.edata['m'] = None\n",
    "            \n",
    "            ## 3. sum the modified messages\n",
    "            g.update_all(\n",
    "                # TODO\n",
    "                None\n",
    "            )\n",
    "            ## TODO: extract the final output into variable \"h_mp\"\n",
    "            h_mp = None\n",
    "            \n",
    "            # End of TODOs\n",
    "            #################################\n",
    "            \n",
    "        \n",
    "        if h_virt is not None:\n",
    "            # Add message from the virtual node\n",
    "            h_mp += h_virt\n",
    "            \n",
    "        # GIN update equation\n",
    "        out = self.mlp((1 + self.eps) * x_node + h_mp)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "3634ae6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GINLayer(\n",
       "  (eps): nn.Parameter\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "    (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=600, out_features=300, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gin = GINLayer(emb_dim)\n",
    "gin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "76b65063",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sum(p.numel() for p in gin.parameters()) == 362101, \"Number of GIN model parameters doesn't match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "935856a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4422, -0.3755,  0.1914,  ...,  0.6874,  0.4973, -0.5937],\n",
       "        [-0.3424, -0.1247,  0.1489,  ...,  0.4517, -0.2917, -0.4845],\n",
       "        [-0.1340, -0.0024,  0.1673,  ...,  0.4772, -0.4345,  0.1413],\n",
       "        ...,\n",
       "        [-0.1618, -0.3611,  0.1262,  ...,  1.0488, -0.5548, -0.4820],\n",
       "        [-0.3787, -0.2358,  0.0531,  ...,  0.7025,  0.7264, -0.5630],\n",
       "        [-0.3787, -0.2358,  0.0531,  ...,  0.7025,  0.7264, -0.5630]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This forward pass should return a tensor without error\n",
    "gin(\n",
    "    batch[0], \n",
    "    AtomEncoder(emb_dim)(batch[0].ndata['feat']), \n",
    "    BondEncoder(emb_dim)(batch[0].edata['feat'])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1551ac39",
   "metadata": {},
   "source": [
    "## Node-level GNN\n",
    "\n",
    "Now that we have the base GIN convolution layer we will create the node-level GNN that stacks GIN layers together to create node embeddings.  This should be similar to node embedding layers we've created in past exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "4c12d21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeGNN(nn.Module):\n",
    "    def __init__(self, emb_dim, num_layers, dropout):\n",
    "        super(NodeGNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.atom_encoder = AtomEncoder(emb_dim)\n",
    "        self.bond_encoder = BondEncoder(emb_dim)\n",
    "        \n",
    "        \"\"\"\n",
    "        Above are the modules that transform the node and edge categorical integer features\n",
    "        into embeddings.  These will be the inputs to the GIN layers.  Below, your \n",
    "        task is to create `num_layers` number of GIN & BatchNorm1d layers, each of size `emb_dim`\n",
    "        \"\"\"\n",
    "        # TODO: add `num_layers` number of GIN and batch norm layers\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        # Atom embeddings\n",
    "        for emb in self.atom_encoder.atom_embedding_list:\n",
    "            nn.init.xavier_uniform_(emb.weight.data)\n",
    "        # Bond embeddings\n",
    "        for emb in self.bond_encoder.bond_embedding_list:\n",
    "            nn.init.xavier_uniform_(emb.weight.data)\n",
    "        \n",
    "        # TODO: reset parameters for the GIN and batch norm layers\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, g):\n",
    "        \"\"\"\n",
    "        Extracts the node/edge feature integers and uses the\n",
    "        Atom/Bond Encoders to get embeddings.  Passes these into \n",
    "        a stack of GIN layers to get node embeddings.\n",
    "        \n",
    "        All hidden layers should have the following pattern:\n",
    "        \n",
    "            (node_embeddings, edge_embeddings) -> GIN -> Batch Norm -> ReLU -> Dropout\n",
    "        \n",
    "        The final layer should have a similar pattern, but with ReLU removed:\n",
    "            (node_embeddings, edge_embeddings) -> GIN -> Batch Norm -> Dropout\n",
    "            \n",
    "        This last output should be returned\n",
    "        \n",
    "        Arguments\n",
    "        ----------\n",
    "        g : DGLGraph\n",
    "            The graph used for message passing\n",
    "        \"\"\"\n",
    "        \n",
    "        # Convert integer categorical features to embeddings\n",
    "        h = self.atom_encoder(g.ndata['feat']) # initial node embeddings\n",
    "        edge_embedding = self.bond_encoder(g.edata['feat']) # edge embeddings\n",
    "        \n",
    "        # TODO: loop through the GIN and BatchNorm layers\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "6ed3aa1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NodeGNN(\n",
       "  (atom_encoder): AtomEncoder(\n",
       "    (atom_embedding_list): ModuleList(\n",
       "      (0): Embedding(119, 300)\n",
       "      (1): Embedding(4, 300)\n",
       "      (2): Embedding(12, 300)\n",
       "      (3): Embedding(12, 300)\n",
       "      (4): Embedding(10, 300)\n",
       "      (5): Embedding(6, 300)\n",
       "      (6): Embedding(6, 300)\n",
       "      (7): Embedding(2, 300)\n",
       "      (8): Embedding(2, 300)\n",
       "    )\n",
       "  )\n",
       "  (bond_encoder): BondEncoder(\n",
       "    (bond_embedding_list): ModuleList(\n",
       "      (0): Embedding(5, 300)\n",
       "      (1): Embedding(6, 300)\n",
       "      (2): Embedding(2, 300)\n",
       "    )\n",
       "  )\n",
       "  (convs): ModuleList(\n",
       "    (0): GINLayer(\n",
       "      (eps): nn.Parameter\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "        (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Linear(in_features=600, out_features=300, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (batch_norms): ModuleList(\n",
       "    (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A throw-away for testing\n",
    "node_gnn = NodeGNN(emb_dim, 1, 0.5)\n",
    "node_gnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "0160192a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sum(p.numel() for p in node_gnn.parameters()) == 418501, \"Number of NodeGNN model parameters doesn't match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "b5bf2f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0547,  0.0000,  0.4401,  ...,  0.0000, -0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.0000,  ...,  3.0385, -0.0000, -0.0376],\n",
       "        [-0.0000, -1.2424, -2.0713,  ..., -1.0885,  0.0000, -0.0000],\n",
       "        ...,\n",
       "        [ 0.7844,  3.8660, -0.0000,  ..., -0.0000, -4.6470,  0.1770],\n",
       "        [-0.0000,  2.1153,  0.0000,  ...,  0.0000, -2.9051,  0.0000],\n",
       "        [-1.4776,  0.0000,  0.6305,  ...,  0.0000, -2.9051,  0.0000]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This should successfully return a tensor\n",
    "node_gnn(batch[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8446d1cf",
   "metadata": {},
   "source": [
    "## Graph-level GNN\n",
    "\n",
    "Finally, we will create a graph-level GNN that uses the node-level GNN as a component, combines the node embeddings of each graph to create a graph-level embedding, and uses this with a Linear layer to generate a prediction for each graph.  The function for converting a set of node embeddings into a graph embedding is sometimes called a \"Readout\" function.  Our readout function will simply be the average of all the node embeddings in the graph:\n",
    "$$\\mathbf{h}_\\mathcal{G} = \\frac{1}{|V_\\mathcal{G}|}\\sum_{j \\in V_\\mathcal{G}} \\mathbf{h}_j$$\n",
    "\n",
    "We'll then make a class prediction for each graph by passing this graph-level embedding through a standard Linear layer:  $\\hat y = \\sigma ( \\mathbf{W} \\cdot \\mathbf{h}_\\mathcal{G} + \\mathbf{b} )$\n",
    "\n",
    "The only real challenge here is related to how we've combined our minibatch of graphs into the \"super-graph\", which results in our node-level GNN returning a tensor of shape `(N_nodes, emb_dim)`, where `N_nodes` count the nodes from *all the graphs in the batch*.  To get the graph-level representation, we'll have to average together the correct rows from that tensor so that only the nodes from the same component graphs are aggregated together.  \n",
    "\n",
    "Luckily, DGL makes this easy with its [read-out ops](https://docs.dgl.ai/api/python/dgl.html#batching-and-reading-out-ops) (e.g., `dgl.mean_nodes`).  The output of this readout function will be of size `(N_graphs, emb_dim)`, where `N_graphs` are the number of graphs in the minibatch that were combined into the \"super-graph\".  Once we have this, we can pass it through the linear layer to generate a final prediction.  Your task is to implement the `forward` pass of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "ff876c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphGNN(nn.Module):\n",
    "    def __init__(self, emb_dim, num_layers, node_cls, dropout):\n",
    "        super(GraphGNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.node_GNN = node_cls(emb_dim, num_layers, dropout)\n",
    "        self.graph_pred_linear = nn.Linear(emb_dim, 1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.node_GNN.reset_parameters()\n",
    "        self.graph_pred_linear.reset_parameters()\n",
    "\n",
    "    def forward(self, g):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        --------------\n",
    "        g : DGLGraph\n",
    "            The batched \"super-graph\" that has combined all the component\n",
    "            graphs from the minibatch.\n",
    "        \"\"\"\n",
    "        # TODO: get the node-level embeddings with self.node_GNN\n",
    "        \n",
    "        # pool the node-level embedding to get graph representations\n",
    "        with g.local_scope():\n",
    "            # TODO: use a DGL readout op to average the nodes in the component graphs\n",
    "            \n",
    "        \n",
    "        # TODO: generate a final prediction by sending the graph-level representation\n",
    "        # through self.graph_pred_linear\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "e39a3b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 5\n",
    "dropout = 0.5\n",
    "\n",
    "model = GraphGNN(emb_dim, num_layers, NodeGNN, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "7d58ae03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphGNN(\n",
       "  (node_GNN): NodeGNN(\n",
       "    (atom_encoder): AtomEncoder(\n",
       "      (atom_embedding_list): ModuleList(\n",
       "        (0): Embedding(119, 300)\n",
       "        (1): Embedding(4, 300)\n",
       "        (2): Embedding(12, 300)\n",
       "        (3): Embedding(12, 300)\n",
       "        (4): Embedding(10, 300)\n",
       "        (5): Embedding(6, 300)\n",
       "        (6): Embedding(6, 300)\n",
       "        (7): Embedding(2, 300)\n",
       "        (8): Embedding(2, 300)\n",
       "      )\n",
       "    )\n",
       "    (convs): ModuleList(\n",
       "      (0): GINLayer(\n",
       "        (eps): nn.Parameter\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Linear(in_features=600, out_features=300, bias=True)\n",
       "        )\n",
       "        (bond_encoder): BondEncoder(\n",
       "          (bond_embedding_list): ModuleList(\n",
       "            (0): Embedding(5, 300)\n",
       "            (1): Embedding(6, 300)\n",
       "            (2): Embedding(2, 300)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): GINLayer(\n",
       "        (eps): nn.Parameter\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Linear(in_features=600, out_features=300, bias=True)\n",
       "        )\n",
       "        (bond_encoder): BondEncoder(\n",
       "          (bond_embedding_list): ModuleList(\n",
       "            (0): Embedding(5, 300)\n",
       "            (1): Embedding(6, 300)\n",
       "            (2): Embedding(2, 300)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): GINLayer(\n",
       "        (eps): nn.Parameter\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Linear(in_features=600, out_features=300, bias=True)\n",
       "        )\n",
       "        (bond_encoder): BondEncoder(\n",
       "          (bond_embedding_list): ModuleList(\n",
       "            (0): Embedding(5, 300)\n",
       "            (1): Embedding(6, 300)\n",
       "            (2): Embedding(2, 300)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): GINLayer(\n",
       "        (eps): nn.Parameter\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Linear(in_features=600, out_features=300, bias=True)\n",
       "        )\n",
       "        (bond_encoder): BondEncoder(\n",
       "          (bond_embedding_list): ModuleList(\n",
       "            (0): Embedding(5, 300)\n",
       "            (1): Embedding(6, 300)\n",
       "            (2): Embedding(2, 300)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): GINLayer(\n",
       "        (eps): nn.Parameter\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Linear(in_features=600, out_features=300, bias=True)\n",
       "        )\n",
       "        (bond_encoder): BondEncoder(\n",
       "          (bond_embedding_list): ModuleList(\n",
       "            (0): Embedding(5, 300)\n",
       "            (1): Embedding(6, 300)\n",
       "            (2): Embedding(2, 300)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (batch_norms): ModuleList(\n",
       "      (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (graph_pred_linear): Linear(in_features=300, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ec696e",
   "metadata": {},
   "source": [
    "When implementing this, it was observed that the PyG/Leaderboard implementation created the `BondEncoder` in the `GIN` layer, which means it created `num_layers` copies of the bond embeddings.  However, the `AtomEncoder` is created only once in the `GNN_node` module ([code](https://github.com/snap-stanford/ogb/blob/master/examples/graphproppred/mol/conv.py#L90)).  Here, I create both encoders once, as I assume the duplicates were a mistake.  As such, the number of parameters tested in the following cell is fewer by this amount from what is published on the leaderboard, as of this writing.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2665d2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sum(p.numel() for p in model.parameters()) == 1869606, \"Number of GNN model parameters doesn't match\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92984960",
   "metadata": {},
   "source": [
    "## Training\n",
    "This will leverage functions defined in the `utils.py` script to standardize training and evaluation.  \n",
    "\n",
    "For faster troubleshooting, note that you should be able to get close to the following performance at the end of the first training epoch.  Since it varies from run to run, I'll include results from 5 separate runs:\n",
    "```\n",
    "Run: 01, Epoch: 01, Loss: 0.1918, Train: 0.6869 AUC, Valid: 0.7028 AUC, Test: 0.7364 AUC\n",
    "Run: 02, Epoch: 01, Loss: 0.1912, Train: 0.6773 AUC, Valid: 0.6854 AUC, Test: 0.6323 AUC\n",
    "Run: 03, Epoch: 01, Loss: 0.1905, Train: 0.7251 AUC, Valid: 0.7114 AUC, Test: 0.6741 AUC\n",
    "Run: 04, Epoch: 01, Loss: 0.1946, Train: 0.7223 AUC, Valid: 0.7206 AUC, Test: 0.6893 AUC\n",
    "Run: 05, Epoch: 01, Loss: 0.1948, Train: 0.7279 AUC, Valid: 0.7034 AUC, Test: 0.6712 AUC\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "50597e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: reduce this to something like 1 run for 5 epochs when developing\n",
    "# and use 10 runs at 100 epochs for the final run\n",
    "N_runs = 10\n",
    "train_args = dict(epochs=100, lr=0.001, eval_steps=1, log_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e388ad1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 01, Epoch: 01, Loss: 0.1918, Train: 0.6869 AUC, Valid: 0.7028 AUC, Test: 0.7364 AUC\n",
      "---\n",
      "Run: 01, Epoch: 02, Loss: 0.1526, Train: 0.7274 AUC, Valid: 0.7311 AUC, Test: 0.7393 AUC\n",
      "---\n",
      "Run: 01, Epoch: 03, Loss: 0.1478, Train: 0.7603 AUC, Valid: 0.7328 AUC, Test: 0.7154 AUC\n",
      "---\n",
      "Run: 01, Epoch: 04, Loss: 0.1449, Train: 0.7542 AUC, Valid: 0.7274 AUC, Test: 0.6996 AUC\n",
      "---\n",
      "Run: 01, Epoch: 05, Loss: 0.1442, Train: 0.7509 AUC, Valid: 0.7358 AUC, Test: 0.6935 AUC\n",
      "---\n",
      "Run: 01, Epoch: 06, Loss: 0.1409, Train: 0.7592 AUC, Valid: 0.7380 AUC, Test: 0.6678 AUC\n",
      "---\n",
      "Run: 01, Epoch: 07, Loss: 0.1398, Train: 0.7697 AUC, Valid: 0.7400 AUC, Test: 0.7274 AUC\n",
      "---\n",
      "Run: 01, Epoch: 08, Loss: 0.1385, Train: 0.7665 AUC, Valid: 0.7575 AUC, Test: 0.7259 AUC\n",
      "---\n",
      "Run: 01, Epoch: 09, Loss: 0.1375, Train: 0.7907 AUC, Valid: 0.7982 AUC, Test: 0.7349 AUC\n",
      "---\n",
      "Run: 01, Epoch: 10, Loss: 0.1357, Train: 0.7814 AUC, Valid: 0.7534 AUC, Test: 0.7140 AUC\n",
      "---\n",
      "Run: 01, Epoch: 11, Loss: 0.1353, Train: 0.7949 AUC, Valid: 0.7630 AUC, Test: 0.7468 AUC\n",
      "---\n",
      "Run: 01, Epoch: 12, Loss: 0.1323, Train: 0.7908 AUC, Valid: 0.7938 AUC, Test: 0.7559 AUC\n",
      "---\n",
      "Run: 01, Epoch: 13, Loss: 0.1327, Train: 0.8168 AUC, Valid: 0.7907 AUC, Test: 0.7376 AUC\n",
      "---\n",
      "Run: 01, Epoch: 14, Loss: 0.1309, Train: 0.8129 AUC, Valid: 0.7871 AUC, Test: 0.7645 AUC\n",
      "---\n",
      "Run: 01, Epoch: 15, Loss: 0.1291, Train: 0.8233 AUC, Valid: 0.8043 AUC, Test: 0.7660 AUC\n",
      "---\n",
      "Run: 01, Epoch: 16, Loss: 0.1275, Train: 0.8262 AUC, Valid: 0.7961 AUC, Test: 0.7250 AUC\n",
      "---\n",
      "Run: 01, Epoch: 17, Loss: 0.1269, Train: 0.8292 AUC, Valid: 0.8137 AUC, Test: 0.7576 AUC\n",
      "---\n",
      "Run: 01, Epoch: 18, Loss: 0.1260, Train: 0.8338 AUC, Valid: 0.7829 AUC, Test: 0.7501 AUC\n",
      "---\n",
      "Run: 01, Epoch: 19, Loss: 0.1244, Train: 0.8107 AUC, Valid: 0.8214 AUC, Test: 0.7189 AUC\n",
      "---\n",
      "Run: 01, Epoch: 20, Loss: 0.1230, Train: 0.8409 AUC, Valid: 0.8112 AUC, Test: 0.7469 AUC\n",
      "---\n",
      "Run: 01, Epoch: 21, Loss: 0.1225, Train: 0.8398 AUC, Valid: 0.7992 AUC, Test: 0.7361 AUC\n",
      "---\n",
      "Run: 01, Epoch: 22, Loss: 0.1219, Train: 0.8496 AUC, Valid: 0.7859 AUC, Test: 0.7559 AUC\n",
      "---\n",
      "Run: 01, Epoch: 23, Loss: 0.1212, Train: 0.8535 AUC, Valid: 0.7968 AUC, Test: 0.7462 AUC\n",
      "---\n",
      "Run: 01, Epoch: 24, Loss: 0.1194, Train: 0.8467 AUC, Valid: 0.8131 AUC, Test: 0.7477 AUC\n",
      "---\n",
      "Run: 01, Epoch: 25, Loss: 0.1190, Train: 0.8477 AUC, Valid: 0.7919 AUC, Test: 0.7427 AUC\n",
      "---\n",
      "Run: 01, Epoch: 26, Loss: 0.1197, Train: 0.8578 AUC, Valid: 0.8153 AUC, Test: 0.7534 AUC\n",
      "---\n",
      "Run: 01, Epoch: 27, Loss: 0.1176, Train: 0.8496 AUC, Valid: 0.7986 AUC, Test: 0.7669 AUC\n",
      "---\n",
      "Run: 01, Epoch: 28, Loss: 0.1166, Train: 0.8568 AUC, Valid: 0.8046 AUC, Test: 0.7714 AUC\n",
      "---\n",
      "Run: 01, Epoch: 29, Loss: 0.1162, Train: 0.8620 AUC, Valid: 0.8096 AUC, Test: 0.7580 AUC\n",
      "---\n",
      "Run: 01, Epoch: 30, Loss: 0.1160, Train: 0.8679 AUC, Valid: 0.8054 AUC, Test: 0.7581 AUC\n",
      "---\n",
      "Run: 01, Epoch: 31, Loss: 0.1156, Train: 0.8724 AUC, Valid: 0.8206 AUC, Test: 0.7624 AUC\n",
      "---\n",
      "Run: 01, Epoch: 32, Loss: 0.1141, Train: 0.8557 AUC, Valid: 0.7855 AUC, Test: 0.7515 AUC\n",
      "---\n",
      "Run: 01, Epoch: 33, Loss: 0.1139, Train: 0.8661 AUC, Valid: 0.7962 AUC, Test: 0.7634 AUC\n",
      "---\n",
      "Run: 01, Epoch: 34, Loss: 0.1145, Train: 0.8644 AUC, Valid: 0.8140 AUC, Test: 0.7616 AUC\n",
      "---\n",
      "Run: 01, Epoch: 35, Loss: 0.1127, Train: 0.8773 AUC, Valid: 0.8058 AUC, Test: 0.7576 AUC\n",
      "---\n",
      "Run: 01, Epoch: 36, Loss: 0.1121, Train: 0.8694 AUC, Valid: 0.8088 AUC, Test: 0.7514 AUC\n",
      "---\n",
      "Run: 01, Epoch: 37, Loss: 0.1114, Train: 0.8845 AUC, Valid: 0.8131 AUC, Test: 0.7682 AUC\n",
      "---\n",
      "Run: 01, Epoch: 38, Loss: 0.1112, Train: 0.8737 AUC, Valid: 0.7753 AUC, Test: 0.7417 AUC\n",
      "---\n",
      "Run: 01, Epoch: 39, Loss: 0.1117, Train: 0.8792 AUC, Valid: 0.7883 AUC, Test: 0.7616 AUC\n",
      "---\n",
      "Run: 01, Epoch: 40, Loss: 0.1099, Train: 0.8764 AUC, Valid: 0.8207 AUC, Test: 0.7654 AUC\n",
      "---\n",
      "Run: 01, Epoch: 41, Loss: 0.1103, Train: 0.8905 AUC, Valid: 0.7924 AUC, Test: 0.7668 AUC\n",
      "---\n",
      "Run: 01, Epoch: 42, Loss: 0.1084, Train: 0.8836 AUC, Valid: 0.8016 AUC, Test: 0.7442 AUC\n",
      "---\n",
      "Run: 01, Epoch: 43, Loss: 0.1100, Train: 0.8909 AUC, Valid: 0.7981 AUC, Test: 0.7450 AUC\n",
      "---\n",
      "Run: 01, Epoch: 44, Loss: 0.1093, Train: 0.8913 AUC, Valid: 0.7932 AUC, Test: 0.7480 AUC\n",
      "---\n",
      "Run: 01, Epoch: 45, Loss: 0.1083, Train: 0.8983 AUC, Valid: 0.7904 AUC, Test: 0.7584 AUC\n",
      "---\n",
      "Run: 01, Epoch: 46, Loss: 0.1077, Train: 0.8955 AUC, Valid: 0.7932 AUC, Test: 0.7532 AUC\n",
      "---\n",
      "Run: 01, Epoch: 47, Loss: 0.1071, Train: 0.8970 AUC, Valid: 0.7743 AUC, Test: 0.7579 AUC\n",
      "---\n",
      "Run: 01, Epoch: 48, Loss: 0.1067, Train: 0.8961 AUC, Valid: 0.7872 AUC, Test: 0.7502 AUC\n",
      "---\n",
      "Run: 01, Epoch: 49, Loss: 0.1053, Train: 0.8966 AUC, Valid: 0.8058 AUC, Test: 0.7566 AUC\n",
      "---\n",
      "Run: 01, Epoch: 50, Loss: 0.1054, Train: 0.9027 AUC, Valid: 0.8003 AUC, Test: 0.7549 AUC\n",
      "---\n",
      "Run: 01, Epoch: 51, Loss: 0.1058, Train: 0.8945 AUC, Valid: 0.7728 AUC, Test: 0.7582 AUC\n",
      "---\n",
      "Run: 01, Epoch: 52, Loss: 0.1036, Train: 0.9022 AUC, Valid: 0.7968 AUC, Test: 0.7753 AUC\n",
      "---\n",
      "Run: 01, Epoch: 53, Loss: 0.1060, Train: 0.9078 AUC, Valid: 0.7935 AUC, Test: 0.7599 AUC\n",
      "---\n",
      "Run: 01, Epoch: 54, Loss: 0.1037, Train: 0.9063 AUC, Valid: 0.8000 AUC, Test: 0.7742 AUC\n",
      "---\n",
      "Run: 01, Epoch: 55, Loss: 0.1028, Train: 0.9062 AUC, Valid: 0.7930 AUC, Test: 0.7612 AUC\n",
      "---\n",
      "Run: 01, Epoch: 56, Loss: 0.1024, Train: 0.9090 AUC, Valid: 0.7726 AUC, Test: 0.7582 AUC\n",
      "---\n",
      "Run: 01, Epoch: 57, Loss: 0.1025, Train: 0.9107 AUC, Valid: 0.8040 AUC, Test: 0.7415 AUC\n",
      "---\n",
      "Run: 01, Epoch: 58, Loss: 0.1025, Train: 0.9107 AUC, Valid: 0.7967 AUC, Test: 0.7553 AUC\n",
      "---\n",
      "Run: 01, Epoch: 59, Loss: 0.1010, Train: 0.9142 AUC, Valid: 0.7777 AUC, Test: 0.7561 AUC\n",
      "---\n",
      "Run: 01, Epoch: 60, Loss: 0.1010, Train: 0.9131 AUC, Valid: 0.7821 AUC, Test: 0.7609 AUC\n",
      "---\n",
      "Run: 01, Epoch: 61, Loss: 0.1010, Train: 0.9167 AUC, Valid: 0.7859 AUC, Test: 0.7560 AUC\n",
      "---\n",
      "Run: 01, Epoch: 62, Loss: 0.1000, Train: 0.9093 AUC, Valid: 0.7810 AUC, Test: 0.7670 AUC\n",
      "---\n",
      "Run: 01, Epoch: 63, Loss: 0.1005, Train: 0.9104 AUC, Valid: 0.7869 AUC, Test: 0.7679 AUC\n",
      "---\n",
      "Run: 01, Epoch: 64, Loss: 0.0987, Train: 0.9169 AUC, Valid: 0.7991 AUC, Test: 0.7663 AUC\n",
      "---\n",
      "Run: 01, Epoch: 65, Loss: 0.1007, Train: 0.9185 AUC, Valid: 0.8028 AUC, Test: 0.7725 AUC\n",
      "---\n",
      "Run: 01, Epoch: 66, Loss: 0.0989, Train: 0.9156 AUC, Valid: 0.7836 AUC, Test: 0.7724 AUC\n",
      "---\n",
      "Run: 01, Epoch: 67, Loss: 0.0981, Train: 0.9237 AUC, Valid: 0.7878 AUC, Test: 0.7705 AUC\n",
      "---\n",
      "Run: 01, Epoch: 68, Loss: 0.0972, Train: 0.9258 AUC, Valid: 0.7795 AUC, Test: 0.7617 AUC\n",
      "---\n",
      "Run: 01, Epoch: 69, Loss: 0.0976, Train: 0.9271 AUC, Valid: 0.7830 AUC, Test: 0.7694 AUC\n",
      "---\n",
      "Run: 01, Epoch: 70, Loss: 0.0975, Train: 0.9264 AUC, Valid: 0.7864 AUC, Test: 0.7645 AUC\n",
      "---\n",
      "Run: 01, Epoch: 71, Loss: 0.0964, Train: 0.9294 AUC, Valid: 0.7816 AUC, Test: 0.7588 AUC\n",
      "---\n",
      "Run: 01, Epoch: 72, Loss: 0.0961, Train: 0.9291 AUC, Valid: 0.7981 AUC, Test: 0.7710 AUC\n",
      "---\n",
      "Run: 01, Epoch: 73, Loss: 0.0965, Train: 0.9168 AUC, Valid: 0.7816 AUC, Test: 0.7618 AUC\n",
      "---\n",
      "Run: 01, Epoch: 74, Loss: 0.0958, Train: 0.9319 AUC, Valid: 0.8012 AUC, Test: 0.7762 AUC\n",
      "---\n",
      "Run: 01, Epoch: 75, Loss: 0.0956, Train: 0.9350 AUC, Valid: 0.7971 AUC, Test: 0.7731 AUC\n",
      "---\n",
      "Run: 01, Epoch: 76, Loss: 0.0942, Train: 0.9335 AUC, Valid: 0.8014 AUC, Test: 0.7576 AUC\n",
      "---\n",
      "Run: 01, Epoch: 77, Loss: 0.0939, Train: 0.9364 AUC, Valid: 0.8075 AUC, Test: 0.7720 AUC\n",
      "---\n",
      "Run: 01, Epoch: 78, Loss: 0.0939, Train: 0.9386 AUC, Valid: 0.7939 AUC, Test: 0.7617 AUC\n",
      "---\n",
      "Run: 01, Epoch: 79, Loss: 0.0931, Train: 0.9328 AUC, Valid: 0.7881 AUC, Test: 0.7574 AUC\n",
      "---\n",
      "Run: 01, Epoch: 80, Loss: 0.0922, Train: 0.9326 AUC, Valid: 0.7852 AUC, Test: 0.7618 AUC\n",
      "---\n",
      "Run: 01, Epoch: 81, Loss: 0.0934, Train: 0.9325 AUC, Valid: 0.7920 AUC, Test: 0.7591 AUC\n",
      "---\n",
      "Run: 01, Epoch: 82, Loss: 0.0921, Train: 0.9390 AUC, Valid: 0.7817 AUC, Test: 0.7702 AUC\n",
      "---\n",
      "Run: 01, Epoch: 83, Loss: 0.0915, Train: 0.9353 AUC, Valid: 0.7799 AUC, Test: 0.7684 AUC\n",
      "---\n",
      "Run: 01, Epoch: 84, Loss: 0.0926, Train: 0.9358 AUC, Valid: 0.7757 AUC, Test: 0.7554 AUC\n",
      "---\n",
      "Run: 01, Epoch: 85, Loss: 0.0908, Train: 0.9442 AUC, Valid: 0.7848 AUC, Test: 0.7755 AUC\n",
      "---\n",
      "Run: 01, Epoch: 86, Loss: 0.0897, Train: 0.9438 AUC, Valid: 0.7916 AUC, Test: 0.7709 AUC\n",
      "---\n",
      "Run: 01, Epoch: 87, Loss: 0.0906, Train: 0.9378 AUC, Valid: 0.7858 AUC, Test: 0.7804 AUC\n",
      "---\n",
      "Run: 01, Epoch: 88, Loss: 0.0899, Train: 0.9411 AUC, Valid: 0.7948 AUC, Test: 0.7741 AUC\n",
      "---\n",
      "Run: 01, Epoch: 89, Loss: 0.0889, Train: 0.9455 AUC, Valid: 0.7915 AUC, Test: 0.7680 AUC\n",
      "---\n",
      "Run: 01, Epoch: 90, Loss: 0.0902, Train: 0.9459 AUC, Valid: 0.7767 AUC, Test: 0.7648 AUC\n",
      "---\n",
      "Run: 01, Epoch: 91, Loss: 0.0880, Train: 0.9469 AUC, Valid: 0.7756 AUC, Test: 0.7586 AUC\n",
      "---\n",
      "Run: 01, Epoch: 92, Loss: 0.0878, Train: 0.9467 AUC, Valid: 0.7970 AUC, Test: 0.7717 AUC\n",
      "---\n",
      "Run: 01, Epoch: 93, Loss: 0.0881, Train: 0.9508 AUC, Valid: 0.7761 AUC, Test: 0.7827 AUC\n",
      "---\n",
      "Run: 01, Epoch: 94, Loss: 0.0886, Train: 0.9439 AUC, Valid: 0.7924 AUC, Test: 0.7890 AUC\n",
      "---\n",
      "Run: 01, Epoch: 95, Loss: 0.0870, Train: 0.9491 AUC, Valid: 0.7886 AUC, Test: 0.7731 AUC\n",
      "---\n",
      "Run: 01, Epoch: 96, Loss: 0.0862, Train: 0.9526 AUC, Valid: 0.7854 AUC, Test: 0.7642 AUC\n",
      "---\n",
      "Run: 01, Epoch: 97, Loss: 0.0856, Train: 0.9508 AUC, Valid: 0.7656 AUC, Test: 0.7596 AUC\n",
      "---\n",
      "Run: 01, Epoch: 98, Loss: 0.0858, Train: 0.9566 AUC, Valid: 0.7885 AUC, Test: 0.7628 AUC\n",
      "---\n",
      "Run: 01, Epoch: 99, Loss: 0.0857, Train: 0.9517 AUC, Valid: 0.7982 AUC, Test: 0.7757 AUC\n",
      "---\n",
      "Run: 01, Epoch: 100, Loss: 0.0850, Train: 0.9508 AUC, Valid: 0.7887 AUC, Test: 0.7572 AUC\n",
      "---\n",
      "Run 01:\n",
      "Highest Train: 95.66\n",
      "Highest Valid: 82.14\n",
      "  Final Train: 81.07\n",
      "   Final Test: 71.89\n",
      "Run: 02, Epoch: 01, Loss: 0.1912, Train: 0.6773 AUC, Valid: 0.6854 AUC, Test: 0.6323 AUC\n",
      "---\n",
      "Run: 02, Epoch: 02, Loss: 0.1525, Train: 0.6940 AUC, Valid: 0.7210 AUC, Test: 0.7222 AUC\n",
      "---\n",
      "Run: 02, Epoch: 03, Loss: 0.1494, Train: 0.7256 AUC, Valid: 0.7087 AUC, Test: 0.6773 AUC\n",
      "---\n",
      "Run: 02, Epoch: 04, Loss: 0.1468, Train: 0.7222 AUC, Valid: 0.7556 AUC, Test: 0.7327 AUC\n",
      "---\n",
      "Run: 02, Epoch: 05, Loss: 0.1444, Train: 0.7668 AUC, Valid: 0.7325 AUC, Test: 0.6551 AUC\n",
      "---\n",
      "Run: 02, Epoch: 06, Loss: 0.1431, Train: 0.7511 AUC, Valid: 0.7654 AUC, Test: 0.7125 AUC\n",
      "---\n",
      "Run: 02, Epoch: 07, Loss: 0.1410, Train: 0.7535 AUC, Valid: 0.7684 AUC, Test: 0.7127 AUC\n",
      "---\n",
      "Run: 02, Epoch: 08, Loss: 0.1383, Train: 0.7800 AUC, Valid: 0.7554 AUC, Test: 0.7171 AUC\n",
      "---\n",
      "Run: 02, Epoch: 09, Loss: 0.1377, Train: 0.7995 AUC, Valid: 0.7830 AUC, Test: 0.7367 AUC\n",
      "---\n",
      "Run: 02, Epoch: 10, Loss: 0.1365, Train: 0.7829 AUC, Valid: 0.7637 AUC, Test: 0.6834 AUC\n",
      "---\n",
      "Run: 02, Epoch: 11, Loss: 0.1339, Train: 0.7950 AUC, Valid: 0.7963 AUC, Test: 0.7098 AUC\n",
      "---\n",
      "Run: 02, Epoch: 12, Loss: 0.1329, Train: 0.8004 AUC, Valid: 0.7372 AUC, Test: 0.7491 AUC\n",
      "---\n",
      "Run: 02, Epoch: 13, Loss: 0.1314, Train: 0.8069 AUC, Valid: 0.7890 AUC, Test: 0.7081 AUC\n",
      "---\n",
      "Run: 02, Epoch: 14, Loss: 0.1324, Train: 0.8183 AUC, Valid: 0.8071 AUC, Test: 0.7397 AUC\n",
      "---\n",
      "Run: 02, Epoch: 15, Loss: 0.1296, Train: 0.8158 AUC, Valid: 0.7953 AUC, Test: 0.7560 AUC\n",
      "---\n",
      "Run: 02, Epoch: 16, Loss: 0.1285, Train: 0.8283 AUC, Valid: 0.7769 AUC, Test: 0.7359 AUC\n",
      "---\n",
      "Run: 02, Epoch: 17, Loss: 0.1266, Train: 0.8173 AUC, Valid: 0.7935 AUC, Test: 0.7527 AUC\n",
      "---\n",
      "Run: 02, Epoch: 18, Loss: 0.1268, Train: 0.8321 AUC, Valid: 0.8087 AUC, Test: 0.7556 AUC\n",
      "---\n",
      "Run: 02, Epoch: 19, Loss: 0.1247, Train: 0.8408 AUC, Valid: 0.8074 AUC, Test: 0.7514 AUC\n",
      "---\n",
      "Run: 02, Epoch: 20, Loss: 0.1246, Train: 0.8445 AUC, Valid: 0.7901 AUC, Test: 0.7578 AUC\n",
      "---\n",
      "Run: 02, Epoch: 21, Loss: 0.1224, Train: 0.8348 AUC, Valid: 0.8139 AUC, Test: 0.7393 AUC\n",
      "---\n",
      "Run: 02, Epoch: 22, Loss: 0.1221, Train: 0.8496 AUC, Valid: 0.7994 AUC, Test: 0.7677 AUC\n",
      "---\n",
      "Run: 02, Epoch: 23, Loss: 0.1208, Train: 0.8543 AUC, Valid: 0.7891 AUC, Test: 0.7651 AUC\n",
      "---\n",
      "Run: 02, Epoch: 49, Loss: 0.1041, Train: 0.8938 AUC, Valid: 0.7772 AUC, Test: 0.7427 AUC\n",
      "---\n",
      "Run: 02, Epoch: 50, Loss: 0.1032, Train: 0.8994 AUC, Valid: 0.7988 AUC, Test: 0.7416 AUC\n",
      "---\n",
      "Run: 02, Epoch: 51, Loss: 0.1022, Train: 0.9027 AUC, Valid: 0.7898 AUC, Test: 0.7527 AUC\n",
      "---\n",
      "Run: 02, Epoch: 52, Loss: 0.1028, Train: 0.9089 AUC, Valid: 0.7798 AUC, Test: 0.7457 AUC\n",
      "---\n",
      "Run: 02, Epoch: 53, Loss: 0.1015, Train: 0.9106 AUC, Valid: 0.7727 AUC, Test: 0.7631 AUC\n",
      "---\n",
      "Run: 02, Epoch: 54, Loss: 0.1018, Train: 0.9128 AUC, Valid: 0.7760 AUC, Test: 0.7637 AUC\n",
      "---\n",
      "Run: 02, Epoch: 55, Loss: 0.1017, Train: 0.9190 AUC, Valid: 0.7856 AUC, Test: 0.7565 AUC\n",
      "---\n",
      "Run: 02, Epoch: 56, Loss: 0.1006, Train: 0.9169 AUC, Valid: 0.7818 AUC, Test: 0.7615 AUC\n",
      "---\n",
      "Run: 02, Epoch: 57, Loss: 0.1001, Train: 0.9159 AUC, Valid: 0.7927 AUC, Test: 0.7696 AUC\n",
      "---\n",
      "Run: 02, Epoch: 58, Loss: 0.1001, Train: 0.9145 AUC, Valid: 0.7506 AUC, Test: 0.7483 AUC\n",
      "---\n",
      "Run: 02, Epoch: 59, Loss: 0.0998, Train: 0.9190 AUC, Valid: 0.7772 AUC, Test: 0.7719 AUC\n",
      "---\n",
      "Run: 02, Epoch: 60, Loss: 0.0987, Train: 0.9241 AUC, Valid: 0.7665 AUC, Test: 0.7612 AUC\n",
      "---\n",
      "Run: 02, Epoch: 61, Loss: 0.0981, Train: 0.9205 AUC, Valid: 0.7830 AUC, Test: 0.7727 AUC\n",
      "---\n",
      "Run: 02, Epoch: 62, Loss: 0.0979, Train: 0.9164 AUC, Valid: 0.7861 AUC, Test: 0.7613 AUC\n",
      "---\n",
      "Run: 02, Epoch: 63, Loss: 0.0985, Train: 0.9253 AUC, Valid: 0.7747 AUC, Test: 0.7581 AUC\n",
      "---\n",
      "Run: 02, Epoch: 64, Loss: 0.0961, Train: 0.9292 AUC, Valid: 0.7687 AUC, Test: 0.7644 AUC\n",
      "---\n",
      "Run: 02, Epoch: 65, Loss: 0.0965, Train: 0.9281 AUC, Valid: 0.7864 AUC, Test: 0.7659 AUC\n",
      "---\n",
      "Run: 02, Epoch: 66, Loss: 0.0954, Train: 0.9260 AUC, Valid: 0.7643 AUC, Test: 0.7687 AUC\n",
      "---\n",
      "Run: 02, Epoch: 67, Loss: 0.0941, Train: 0.9254 AUC, Valid: 0.7623 AUC, Test: 0.7602 AUC\n",
      "---\n",
      "Run: 02, Epoch: 68, Loss: 0.0949, Train: 0.9262 AUC, Valid: 0.7890 AUC, Test: 0.7661 AUC\n",
      "---\n",
      "Run: 02, Epoch: 69, Loss: 0.0953, Train: 0.9347 AUC, Valid: 0.7686 AUC, Test: 0.7675 AUC\n",
      "---\n",
      "Run: 02, Epoch: 70, Loss: 0.0942, Train: 0.9350 AUC, Valid: 0.7921 AUC, Test: 0.7617 AUC\n",
      "---\n",
      "Run: 02, Epoch: 71, Loss: 0.0933, Train: 0.9340 AUC, Valid: 0.7860 AUC, Test: 0.7500 AUC\n",
      "---\n",
      "Run: 02, Epoch: 72, Loss: 0.0927, Train: 0.9322 AUC, Valid: 0.7950 AUC, Test: 0.7543 AUC\n",
      "---\n",
      "Run: 02, Epoch: 73, Loss: 0.0925, Train: 0.9342 AUC, Valid: 0.7736 AUC, Test: 0.7807 AUC\n",
      "---\n",
      "Run: 02, Epoch: 74, Loss: 0.0918, Train: 0.9374 AUC, Valid: 0.7999 AUC, Test: 0.7629 AUC\n",
      "---\n",
      "Run: 02, Epoch: 75, Loss: 0.0926, Train: 0.9279 AUC, Valid: 0.7891 AUC, Test: 0.7494 AUC\n",
      "---\n",
      "Run: 02, Epoch: 76, Loss: 0.0918, Train: 0.9330 AUC, Valid: 0.7813 AUC, Test: 0.7516 AUC\n",
      "---\n",
      "Run: 02, Epoch: 77, Loss: 0.0919, Train: 0.9394 AUC, Valid: 0.7656 AUC, Test: 0.7582 AUC\n",
      "---\n",
      "Run: 02, Epoch: 78, Loss: 0.0911, Train: 0.9370 AUC, Valid: 0.7748 AUC, Test: 0.7647 AUC\n",
      "---\n",
      "Run: 02, Epoch: 79, Loss: 0.0909, Train: 0.9374 AUC, Valid: 0.7970 AUC, Test: 0.7585 AUC\n",
      "---\n",
      "Run: 02, Epoch: 80, Loss: 0.0909, Train: 0.9327 AUC, Valid: 0.7576 AUC, Test: 0.7529 AUC\n",
      "---\n",
      "Run: 02, Epoch: 81, Loss: 0.0918, Train: 0.9358 AUC, Valid: 0.7804 AUC, Test: 0.7727 AUC\n",
      "---\n",
      "Run: 02, Epoch: 82, Loss: 0.0894, Train: 0.9394 AUC, Valid: 0.7902 AUC, Test: 0.7646 AUC\n",
      "---\n",
      "Run: 02, Epoch: 83, Loss: 0.0893, Train: 0.9386 AUC, Valid: 0.7791 AUC, Test: 0.7592 AUC\n",
      "---\n",
      "Run: 02, Epoch: 84, Loss: 0.0891, Train: 0.9401 AUC, Valid: 0.7581 AUC, Test: 0.7602 AUC\n",
      "---\n",
      "Run: 02, Epoch: 85, Loss: 0.0886, Train: 0.9438 AUC, Valid: 0.7634 AUC, Test: 0.7667 AUC\n",
      "---\n",
      "Run: 02, Epoch: 86, Loss: 0.0883, Train: 0.9373 AUC, Valid: 0.7723 AUC, Test: 0.7647 AUC\n",
      "---\n",
      "Run: 02, Epoch: 87, Loss: 0.0889, Train: 0.9453 AUC, Valid: 0.7623 AUC, Test: 0.7621 AUC\n",
      "---\n",
      "Run: 02, Epoch: 88, Loss: 0.0873, Train: 0.9425 AUC, Valid: 0.7707 AUC, Test: 0.7381 AUC\n",
      "---\n",
      "Run: 02, Epoch: 89, Loss: 0.0881, Train: 0.9444 AUC, Valid: 0.7731 AUC, Test: 0.7603 AUC\n",
      "---\n",
      "Run: 02, Epoch: 90, Loss: 0.0875, Train: 0.9484 AUC, Valid: 0.7818 AUC, Test: 0.7508 AUC\n",
      "---\n",
      "Run: 02, Epoch: 91, Loss: 0.0856, Train: 0.9475 AUC, Valid: 0.7683 AUC, Test: 0.7562 AUC\n",
      "---\n",
      "Run: 02, Epoch: 92, Loss: 0.0852, Train: 0.9497 AUC, Valid: 0.7798 AUC, Test: 0.7551 AUC\n",
      "---\n",
      "Run: 02, Epoch: 93, Loss: 0.0856, Train: 0.9475 AUC, Valid: 0.7725 AUC, Test: 0.7579 AUC\n",
      "---\n",
      "Run: 02, Epoch: 94, Loss: 0.0844, Train: 0.9444 AUC, Valid: 0.7693 AUC, Test: 0.7450 AUC\n",
      "---\n",
      "Run: 02, Epoch: 95, Loss: 0.0846, Train: 0.9488 AUC, Valid: 0.7836 AUC, Test: 0.7450 AUC\n",
      "---\n",
      "Run: 02, Epoch: 96, Loss: 0.0853, Train: 0.9464 AUC, Valid: 0.7608 AUC, Test: 0.7501 AUC\n",
      "---\n",
      "Run: 02, Epoch: 97, Loss: 0.0847, Train: 0.9560 AUC, Valid: 0.7793 AUC, Test: 0.7634 AUC\n",
      "---\n",
      "Run: 02, Epoch: 98, Loss: 0.0849, Train: 0.9512 AUC, Valid: 0.7662 AUC, Test: 0.7635 AUC\n",
      "---\n",
      "Run: 02, Epoch: 99, Loss: 0.0845, Train: 0.9526 AUC, Valid: 0.7709 AUC, Test: 0.7686 AUC\n",
      "---\n",
      "Run: 02, Epoch: 100, Loss: 0.0835, Train: 0.9516 AUC, Valid: 0.7721 AUC, Test: 0.7625 AUC\n",
      "---\n",
      "Run 02:\n",
      "Highest Train: 95.60\n",
      "Highest Valid: 81.39\n",
      "  Final Train: 83.48\n",
      "   Final Test: 73.93\n",
      "Run: 03, Epoch: 01, Loss: 0.1905, Train: 0.7251 AUC, Valid: 0.7114 AUC, Test: 0.6741 AUC\n",
      "---\n",
      "Run: 03, Epoch: 02, Loss: 0.1500, Train: 0.7044 AUC, Valid: 0.7118 AUC, Test: 0.7299 AUC\n",
      "---\n",
      "Run: 03, Epoch: 03, Loss: 0.1460, Train: 0.6979 AUC, Valid: 0.6580 AUC, Test: 0.6947 AUC\n",
      "---\n",
      "Run: 03, Epoch: 04, Loss: 0.1461, Train: 0.7389 AUC, Valid: 0.7447 AUC, Test: 0.7276 AUC\n",
      "---\n",
      "Run: 03, Epoch: 05, Loss: 0.1425, Train: 0.7758 AUC, Valid: 0.7746 AUC, Test: 0.7270 AUC\n",
      "---\n",
      "Run: 03, Epoch: 06, Loss: 0.1422, Train: 0.7628 AUC, Valid: 0.7590 AUC, Test: 0.7216 AUC\n",
      "---\n",
      "Run: 03, Epoch: 07, Loss: 0.1394, Train: 0.7747 AUC, Valid: 0.7589 AUC, Test: 0.7419 AUC\n",
      "---\n",
      "Run: 03, Epoch: 08, Loss: 0.1384, Train: 0.7844 AUC, Valid: 0.8051 AUC, Test: 0.7467 AUC\n",
      "---\n",
      "Run: 03, Epoch: 09, Loss: 0.1368, Train: 0.7851 AUC, Valid: 0.7491 AUC, Test: 0.7324 AUC\n",
      "---\n",
      "Run: 03, Epoch: 10, Loss: 0.1348, Train: 0.7905 AUC, Valid: 0.7775 AUC, Test: 0.7342 AUC\n",
      "---\n",
      "Run: 03, Epoch: 11, Loss: 0.1358, Train: 0.8039 AUC, Valid: 0.7997 AUC, Test: 0.7422 AUC\n",
      "---\n",
      "Run: 03, Epoch: 12, Loss: 0.1322, Train: 0.8058 AUC, Valid: 0.7653 AUC, Test: 0.7260 AUC\n",
      "---\n",
      "Run: 03, Epoch: 13, Loss: 0.1310, Train: 0.8095 AUC, Valid: 0.8037 AUC, Test: 0.7512 AUC\n",
      "---\n",
      "Run: 03, Epoch: 14, Loss: 0.1310, Train: 0.8178 AUC, Valid: 0.8037 AUC, Test: 0.7615 AUC\n",
      "---\n",
      "Run: 03, Epoch: 15, Loss: 0.1312, Train: 0.8193 AUC, Valid: 0.7933 AUC, Test: 0.7579 AUC\n",
      "---\n",
      "Run: 03, Epoch: 16, Loss: 0.1286, Train: 0.8176 AUC, Valid: 0.7991 AUC, Test: 0.7231 AUC\n",
      "---\n",
      "Run: 03, Epoch: 17, Loss: 0.1272, Train: 0.8162 AUC, Valid: 0.7858 AUC, Test: 0.7393 AUC\n",
      "---\n",
      "Run: 03, Epoch: 18, Loss: 0.1265, Train: 0.8230 AUC, Valid: 0.7703 AUC, Test: 0.7040 AUC\n",
      "---\n",
      "Run: 03, Epoch: 19, Loss: 0.1268, Train: 0.8368 AUC, Valid: 0.7909 AUC, Test: 0.7558 AUC\n",
      "---\n",
      "Run: 03, Epoch: 20, Loss: 0.1245, Train: 0.8382 AUC, Valid: 0.7810 AUC, Test: 0.7603 AUC\n",
      "---\n",
      "Run: 03, Epoch: 21, Loss: 0.1233, Train: 0.8355 AUC, Valid: 0.7786 AUC, Test: 0.7341 AUC\n",
      "---\n",
      "Run: 03, Epoch: 22, Loss: 0.1239, Train: 0.8397 AUC, Valid: 0.7837 AUC, Test: 0.7495 AUC\n",
      "---\n",
      "Run: 03, Epoch: 23, Loss: 0.1221, Train: 0.8496 AUC, Valid: 0.7835 AUC, Test: 0.7727 AUC\n",
      "---\n",
      "Run: 03, Epoch: 24, Loss: 0.1203, Train: 0.8491 AUC, Valid: 0.7812 AUC, Test: 0.7581 AUC\n",
      "---\n",
      "Run: 03, Epoch: 25, Loss: 0.1190, Train: 0.8449 AUC, Valid: 0.7691 AUC, Test: 0.7343 AUC\n",
      "---\n",
      "Run: 03, Epoch: 26, Loss: 0.1213, Train: 0.8545 AUC, Valid: 0.7838 AUC, Test: 0.7527 AUC\n",
      "---\n",
      "Run: 03, Epoch: 27, Loss: 0.1186, Train: 0.8590 AUC, Valid: 0.7616 AUC, Test: 0.7432 AUC\n",
      "---\n",
      "Run: 03, Epoch: 28, Loss: 0.1180, Train: 0.8637 AUC, Valid: 0.7717 AUC, Test: 0.7517 AUC\n",
      "---\n",
      "Run: 03, Epoch: 29, Loss: 0.1176, Train: 0.8623 AUC, Valid: 0.7871 AUC, Test: 0.7595 AUC\n",
      "---\n",
      "Run: 03, Epoch: 30, Loss: 0.1162, Train: 0.8653 AUC, Valid: 0.7966 AUC, Test: 0.7458 AUC\n",
      "---\n",
      "Run: 03, Epoch: 31, Loss: 0.1171, Train: 0.8694 AUC, Valid: 0.7984 AUC, Test: 0.7484 AUC\n",
      "---\n",
      "Run: 03, Epoch: 32, Loss: 0.1158, Train: 0.8621 AUC, Valid: 0.7867 AUC, Test: 0.7486 AUC\n",
      "---\n",
      "Run: 03, Epoch: 33, Loss: 0.1151, Train: 0.8665 AUC, Valid: 0.7775 AUC, Test: 0.7478 AUC\n",
      "---\n",
      "Run: 03, Epoch: 34, Loss: 0.1148, Train: 0.8696 AUC, Valid: 0.7950 AUC, Test: 0.7579 AUC\n",
      "---\n",
      "Run: 03, Epoch: 35, Loss: 0.1134, Train: 0.8743 AUC, Valid: 0.7597 AUC, Test: 0.7392 AUC\n",
      "---\n",
      "Run: 03, Epoch: 36, Loss: 0.1134, Train: 0.8731 AUC, Valid: 0.7700 AUC, Test: 0.7547 AUC\n",
      "---\n",
      "Run: 03, Epoch: 37, Loss: 0.1138, Train: 0.8762 AUC, Valid: 0.8037 AUC, Test: 0.7617 AUC\n",
      "---\n",
      "Run: 03, Epoch: 38, Loss: 0.1130, Train: 0.8795 AUC, Valid: 0.8080 AUC, Test: 0.7405 AUC\n",
      "---\n",
      "Run: 03, Epoch: 39, Loss: 0.1110, Train: 0.8740 AUC, Valid: 0.7583 AUC, Test: 0.7587 AUC\n",
      "---\n",
      "Run: 03, Epoch: 40, Loss: 0.1117, Train: 0.8796 AUC, Valid: 0.7740 AUC, Test: 0.7451 AUC\n",
      "---\n",
      "Run: 03, Epoch: 41, Loss: 0.1099, Train: 0.8805 AUC, Valid: 0.7994 AUC, Test: 0.7444 AUC\n",
      "---\n",
      "Run: 03, Epoch: 42, Loss: 0.1105, Train: 0.8798 AUC, Valid: 0.7867 AUC, Test: 0.7428 AUC\n",
      "---\n",
      "Run: 03, Epoch: 43, Loss: 0.1097, Train: 0.8884 AUC, Valid: 0.7889 AUC, Test: 0.7517 AUC\n",
      "---\n",
      "Run: 03, Epoch: 44, Loss: 0.1090, Train: 0.8848 AUC, Valid: 0.7665 AUC, Test: 0.7582 AUC\n",
      "---\n",
      "Run: 03, Epoch: 45, Loss: 0.1081, Train: 0.8951 AUC, Valid: 0.8009 AUC, Test: 0.7467 AUC\n",
      "---\n",
      "Run: 03, Epoch: 46, Loss: 0.1067, Train: 0.8916 AUC, Valid: 0.7845 AUC, Test: 0.7622 AUC\n",
      "---\n",
      "Run: 03, Epoch: 47, Loss: 0.1076, Train: 0.8933 AUC, Valid: 0.8113 AUC, Test: 0.7538 AUC\n",
      "---\n",
      "Run: 03, Epoch: 48, Loss: 0.1062, Train: 0.8872 AUC, Valid: 0.7924 AUC, Test: 0.7415 AUC\n",
      "---\n",
      "Run: 03, Epoch: 49, Loss: 0.1069, Train: 0.9026 AUC, Valid: 0.7947 AUC, Test: 0.7464 AUC\n",
      "---\n",
      "Run: 03, Epoch: 50, Loss: 0.1050, Train: 0.8871 AUC, Valid: 0.7901 AUC, Test: 0.7589 AUC\n",
      "---\n",
      "Run: 03, Epoch: 51, Loss: 0.1050, Train: 0.8983 AUC, Valid: 0.8050 AUC, Test: 0.7329 AUC\n",
      "---\n",
      "Run: 03, Epoch: 52, Loss: 0.1054, Train: 0.8999 AUC, Valid: 0.7777 AUC, Test: 0.7488 AUC\n",
      "---\n",
      "Run: 03, Epoch: 53, Loss: 0.1049, Train: 0.9046 AUC, Valid: 0.8001 AUC, Test: 0.7563 AUC\n",
      "---\n",
      "Run: 03, Epoch: 54, Loss: 0.1036, Train: 0.8936 AUC, Valid: 0.7583 AUC, Test: 0.7615 AUC\n",
      "---\n",
      "Run: 03, Epoch: 55, Loss: 0.1023, Train: 0.9070 AUC, Valid: 0.7757 AUC, Test: 0.7612 AUC\n",
      "---\n",
      "Run: 03, Epoch: 56, Loss: 0.1034, Train: 0.9062 AUC, Valid: 0.7616 AUC, Test: 0.7369 AUC\n",
      "---\n",
      "Run: 03, Epoch: 57, Loss: 0.1035, Train: 0.8945 AUC, Valid: 0.7708 AUC, Test: 0.7339 AUC\n",
      "---\n",
      "Run: 03, Epoch: 58, Loss: 0.1028, Train: 0.9027 AUC, Valid: 0.7754 AUC, Test: 0.7375 AUC\n",
      "---\n",
      "Run: 03, Epoch: 59, Loss: 0.1027, Train: 0.9091 AUC, Valid: 0.7927 AUC, Test: 0.7506 AUC\n",
      "---\n",
      "Run: 03, Epoch: 60, Loss: 0.1010, Train: 0.9137 AUC, Valid: 0.7524 AUC, Test: 0.7582 AUC\n",
      "---\n",
      "Run: 03, Epoch: 61, Loss: 0.0996, Train: 0.9098 AUC, Valid: 0.7976 AUC, Test: 0.7535 AUC\n",
      "---\n",
      "Run: 03, Epoch: 62, Loss: 0.1004, Train: 0.9127 AUC, Valid: 0.7682 AUC, Test: 0.7807 AUC\n",
      "---\n",
      "Run: 03, Epoch: 63, Loss: 0.1004, Train: 0.9129 AUC, Valid: 0.7854 AUC, Test: 0.7539 AUC\n",
      "---\n",
      "Run: 03, Epoch: 64, Loss: 0.1007, Train: 0.9172 AUC, Valid: 0.8154 AUC, Test: 0.7603 AUC\n",
      "---\n",
      "Run: 03, Epoch: 65, Loss: 0.0994, Train: 0.9021 AUC, Valid: 0.7735 AUC, Test: 0.7558 AUC\n",
      "---\n",
      "Run: 03, Epoch: 66, Loss: 0.0988, Train: 0.9191 AUC, Valid: 0.7693 AUC, Test: 0.7529 AUC\n",
      "---\n",
      "Run: 03, Epoch: 67, Loss: 0.1000, Train: 0.9146 AUC, Valid: 0.7687 AUC, Test: 0.7564 AUC\n",
      "---\n",
      "Run: 03, Epoch: 68, Loss: 0.0979, Train: 0.9251 AUC, Valid: 0.7748 AUC, Test: 0.7595 AUC\n",
      "---\n",
      "Run: 03, Epoch: 69, Loss: 0.0976, Train: 0.9246 AUC, Valid: 0.7809 AUC, Test: 0.7554 AUC\n",
      "---\n",
      "Run: 03, Epoch: 70, Loss: 0.0969, Train: 0.9221 AUC, Valid: 0.7907 AUC, Test: 0.7533 AUC\n",
      "---\n",
      "Run: 03, Epoch: 71, Loss: 0.0976, Train: 0.9234 AUC, Valid: 0.7765 AUC, Test: 0.7558 AUC\n",
      "---\n",
      "Run: 03, Epoch: 72, Loss: 0.0973, Train: 0.9268 AUC, Valid: 0.7799 AUC, Test: 0.7627 AUC\n",
      "---\n",
      "Run: 03, Epoch: 80, Loss: 0.0925, Train: 0.9348 AUC, Valid: 0.7715 AUC, Test: 0.7632 AUC\n",
      "---\n",
      "Run: 03, Epoch: 81, Loss: 0.0939, Train: 0.9269 AUC, Valid: 0.7791 AUC, Test: 0.7696 AUC\n",
      "---\n",
      "Run: 03, Epoch: 82, Loss: 0.0917, Train: 0.9339 AUC, Valid: 0.7862 AUC, Test: 0.7614 AUC\n",
      "---\n",
      "Run: 03, Epoch: 83, Loss: 0.0919, Train: 0.9320 AUC, Valid: 0.7814 AUC, Test: 0.7677 AUC\n",
      "---\n",
      "Run: 03, Epoch: 84, Loss: 0.0924, Train: 0.9436 AUC, Valid: 0.7755 AUC, Test: 0.7718 AUC\n",
      "---\n",
      "Run: 03, Epoch: 85, Loss: 0.0905, Train: 0.9402 AUC, Valid: 0.7776 AUC, Test: 0.7599 AUC\n",
      "---\n",
      "Run: 03, Epoch: 86, Loss: 0.0913, Train: 0.9422 AUC, Valid: 0.7812 AUC, Test: 0.7609 AUC\n",
      "---\n",
      "Run: 03, Epoch: 87, Loss: 0.0914, Train: 0.9410 AUC, Valid: 0.7728 AUC, Test: 0.7732 AUC\n",
      "---\n",
      "Run: 03, Epoch: 88, Loss: 0.0910, Train: 0.9366 AUC, Valid: 0.7745 AUC, Test: 0.7607 AUC\n",
      "---\n",
      "Run: 03, Epoch: 89, Loss: 0.0926, Train: 0.9427 AUC, Valid: 0.8053 AUC, Test: 0.7858 AUC\n",
      "---\n",
      "Run: 03, Epoch: 90, Loss: 0.0906, Train: 0.9461 AUC, Valid: 0.7835 AUC, Test: 0.7675 AUC\n",
      "---\n",
      "Run: 03, Epoch: 91, Loss: 0.0897, Train: 0.9429 AUC, Valid: 0.7972 AUC, Test: 0.7710 AUC\n",
      "---\n",
      "Run: 03, Epoch: 92, Loss: 0.0894, Train: 0.9468 AUC, Valid: 0.7870 AUC, Test: 0.7642 AUC\n",
      "---\n",
      "Run: 03, Epoch: 93, Loss: 0.0900, Train: 0.9464 AUC, Valid: 0.7708 AUC, Test: 0.7713 AUC\n",
      "---\n",
      "Run: 03, Epoch: 94, Loss: 0.0886, Train: 0.9503 AUC, Valid: 0.7689 AUC, Test: 0.7595 AUC\n",
      "---\n",
      "Run: 03, Epoch: 95, Loss: 0.0895, Train: 0.9441 AUC, Valid: 0.7823 AUC, Test: 0.7692 AUC\n",
      "---\n",
      "Run: 03, Epoch: 96, Loss: 0.0879, Train: 0.9515 AUC, Valid: 0.7760 AUC, Test: 0.7739 AUC\n",
      "---\n",
      "Run: 03, Epoch: 97, Loss: 0.0883, Train: 0.9472 AUC, Valid: 0.7856 AUC, Test: 0.7744 AUC\n",
      "---\n",
      "Run: 03, Epoch: 98, Loss: 0.0874, Train: 0.9469 AUC, Valid: 0.7889 AUC, Test: 0.7651 AUC\n",
      "---\n",
      "Run: 03, Epoch: 99, Loss: 0.0878, Train: 0.9484 AUC, Valid: 0.7920 AUC, Test: 0.7720 AUC\n",
      "---\n",
      "Run: 03, Epoch: 100, Loss: 0.0864, Train: 0.9488 AUC, Valid: 0.7650 AUC, Test: 0.7585 AUC\n",
      "---\n",
      "Run 03:\n",
      "Highest Train: 95.15\n",
      "Highest Valid: 81.54\n",
      "  Final Train: 91.72\n",
      "   Final Test: 76.03\n",
      "Run: 04, Epoch: 01, Loss: 0.1946, Train: 0.7223 AUC, Valid: 0.7206 AUC, Test: 0.6893 AUC\n",
      "---\n",
      "Run: 04, Epoch: 02, Loss: 0.1514, Train: 0.7318 AUC, Valid: 0.7223 AUC, Test: 0.7013 AUC\n",
      "---\n",
      "Run: 04, Epoch: 03, Loss: 0.1479, Train: 0.7333 AUC, Valid: 0.7170 AUC, Test: 0.7259 AUC\n",
      "---\n",
      "Run: 04, Epoch: 04, Loss: 0.1460, Train: 0.7417 AUC, Valid: 0.7427 AUC, Test: 0.7262 AUC\n",
      "---\n",
      "Run: 04, Epoch: 05, Loss: 0.1431, Train: 0.7592 AUC, Valid: 0.7079 AUC, Test: 0.7005 AUC\n",
      "---\n",
      "Run: 04, Epoch: 06, Loss: 0.1423, Train: 0.7597 AUC, Valid: 0.7428 AUC, Test: 0.6915 AUC\n",
      "---\n",
      "Run: 04, Epoch: 07, Loss: 0.1393, Train: 0.7670 AUC, Valid: 0.7475 AUC, Test: 0.6995 AUC\n",
      "---\n",
      "Run: 04, Epoch: 08, Loss: 0.1393, Train: 0.7852 AUC, Valid: 0.7579 AUC, Test: 0.7274 AUC\n",
      "---\n",
      "Run: 04, Epoch: 09, Loss: 0.1381, Train: 0.7900 AUC, Valid: 0.7624 AUC, Test: 0.7348 AUC\n",
      "---\n",
      "Run: 04, Epoch: 10, Loss: 0.1367, Train: 0.7971 AUC, Valid: 0.7856 AUC, Test: 0.7175 AUC\n",
      "---\n",
      "Run: 04, Epoch: 11, Loss: 0.1346, Train: 0.7999 AUC, Valid: 0.7945 AUC, Test: 0.7405 AUC\n",
      "---\n",
      "Run: 04, Epoch: 12, Loss: 0.1339, Train: 0.8034 AUC, Valid: 0.7826 AUC, Test: 0.7194 AUC\n",
      "---\n",
      "Run: 04, Epoch: 13, Loss: 0.1316, Train: 0.7960 AUC, Valid: 0.7787 AUC, Test: 0.7291 AUC\n",
      "---\n",
      "Run: 04, Epoch: 14, Loss: 0.1309, Train: 0.8009 AUC, Valid: 0.7858 AUC, Test: 0.7042 AUC\n",
      "---\n",
      "Run: 04, Epoch: 15, Loss: 0.1303, Train: 0.8100 AUC, Valid: 0.7832 AUC, Test: 0.7322 AUC\n",
      "---\n",
      "Run: 04, Epoch: 16, Loss: 0.1304, Train: 0.8110 AUC, Valid: 0.7737 AUC, Test: 0.7204 AUC\n",
      "---\n",
      "Run: 04, Epoch: 17, Loss: 0.1276, Train: 0.8240 AUC, Valid: 0.8010 AUC, Test: 0.7257 AUC\n",
      "---\n",
      "Run: 04, Epoch: 18, Loss: 0.1276, Train: 0.8151 AUC, Valid: 0.7962 AUC, Test: 0.7276 AUC\n",
      "---\n",
      "Run: 04, Epoch: 19, Loss: 0.1266, Train: 0.8266 AUC, Valid: 0.8005 AUC, Test: 0.7277 AUC\n",
      "---\n",
      "Run: 04, Epoch: 20, Loss: 0.1258, Train: 0.8348 AUC, Valid: 0.7880 AUC, Test: 0.7443 AUC\n",
      "---\n",
      "Run: 04, Epoch: 21, Loss: 0.1238, Train: 0.8358 AUC, Valid: 0.8024 AUC, Test: 0.7298 AUC\n",
      "---\n",
      "Run: 04, Epoch: 22, Loss: 0.1228, Train: 0.8346 AUC, Valid: 0.8010 AUC, Test: 0.6980 AUC\n",
      "---\n",
      "Run: 04, Epoch: 23, Loss: 0.1219, Train: 0.8381 AUC, Valid: 0.7862 AUC, Test: 0.7394 AUC\n",
      "---\n",
      "Run: 04, Epoch: 24, Loss: 0.1207, Train: 0.8474 AUC, Valid: 0.8226 AUC, Test: 0.7296 AUC\n",
      "---\n",
      "Run: 04, Epoch: 25, Loss: 0.1194, Train: 0.8391 AUC, Valid: 0.8054 AUC, Test: 0.6981 AUC\n",
      "---\n",
      "Run: 04, Epoch: 26, Loss: 0.1187, Train: 0.8567 AUC, Valid: 0.8321 AUC, Test: 0.7571 AUC\n",
      "---\n",
      "Run: 04, Epoch: 27, Loss: 0.1189, Train: 0.8555 AUC, Valid: 0.8000 AUC, Test: 0.7420 AUC\n",
      "---\n",
      "Run: 04, Epoch: 28, Loss: 0.1170, Train: 0.8553 AUC, Valid: 0.7988 AUC, Test: 0.7680 AUC\n",
      "---\n",
      "Run: 04, Epoch: 29, Loss: 0.1179, Train: 0.8612 AUC, Valid: 0.7994 AUC, Test: 0.7422 AUC\n",
      "---\n",
      "Run: 04, Epoch: 30, Loss: 0.1166, Train: 0.8625 AUC, Valid: 0.8140 AUC, Test: 0.7507 AUC\n",
      "---\n",
      "Run: 04, Epoch: 31, Loss: 0.1150, Train: 0.8668 AUC, Valid: 0.8112 AUC, Test: 0.7557 AUC\n",
      "---\n",
      "Run: 04, Epoch: 32, Loss: 0.1142, Train: 0.8632 AUC, Valid: 0.8088 AUC, Test: 0.7597 AUC\n",
      "---\n",
      "Run: 04, Epoch: 33, Loss: 0.1141, Train: 0.8680 AUC, Valid: 0.8231 AUC, Test: 0.7512 AUC\n",
      "---\n",
      "Run: 04, Epoch: 34, Loss: 0.1132, Train: 0.8712 AUC, Valid: 0.8118 AUC, Test: 0.7495 AUC\n",
      "---\n",
      "Run: 04, Epoch: 35, Loss: 0.1127, Train: 0.8728 AUC, Valid: 0.8233 AUC, Test: 0.7490 AUC\n",
      "---\n",
      "Run: 04, Epoch: 36, Loss: 0.1118, Train: 0.8773 AUC, Valid: 0.8110 AUC, Test: 0.7586 AUC\n",
      "---\n",
      "Run: 04, Epoch: 37, Loss: 0.1107, Train: 0.8795 AUC, Valid: 0.8108 AUC, Test: 0.7507 AUC\n",
      "---\n",
      "Run: 04, Epoch: 38, Loss: 0.1114, Train: 0.8803 AUC, Valid: 0.8292 AUC, Test: 0.7635 AUC\n",
      "---\n",
      "Run: 04, Epoch: 39, Loss: 0.1111, Train: 0.8777 AUC, Valid: 0.7960 AUC, Test: 0.7629 AUC\n",
      "---\n",
      "Run: 04, Epoch: 40, Loss: 0.1095, Train: 0.8778 AUC, Valid: 0.8229 AUC, Test: 0.7289 AUC\n",
      "---\n",
      "Run: 04, Epoch: 41, Loss: 0.1096, Train: 0.8835 AUC, Valid: 0.8322 AUC, Test: 0.7346 AUC\n",
      "---\n",
      "Run: 04, Epoch: 42, Loss: 0.1090, Train: 0.8893 AUC, Valid: 0.8144 AUC, Test: 0.7564 AUC\n",
      "---\n",
      "Run: 04, Epoch: 43, Loss: 0.1082, Train: 0.8901 AUC, Valid: 0.8119 AUC, Test: 0.7411 AUC\n",
      "---\n",
      "Run: 04, Epoch: 44, Loss: 0.1063, Train: 0.8814 AUC, Valid: 0.8089 AUC, Test: 0.7230 AUC\n",
      "---\n",
      "Run: 04, Epoch: 45, Loss: 0.1086, Train: 0.8969 AUC, Valid: 0.8340 AUC, Test: 0.7583 AUC\n",
      "---\n",
      "Run: 04, Epoch: 46, Loss: 0.1052, Train: 0.8906 AUC, Valid: 0.8110 AUC, Test: 0.7486 AUC\n",
      "---\n",
      "Run: 04, Epoch: 47, Loss: 0.1051, Train: 0.8965 AUC, Valid: 0.8266 AUC, Test: 0.7620 AUC\n",
      "---\n",
      "Run: 04, Epoch: 48, Loss: 0.1064, Train: 0.8978 AUC, Valid: 0.8169 AUC, Test: 0.7512 AUC\n",
      "---\n",
      "Run: 04, Epoch: 49, Loss: 0.1057, Train: 0.9001 AUC, Valid: 0.8133 AUC, Test: 0.7503 AUC\n",
      "---\n",
      "Run: 04, Epoch: 50, Loss: 0.1044, Train: 0.8979 AUC, Valid: 0.8192 AUC, Test: 0.7515 AUC\n",
      "---\n",
      "Run: 04, Epoch: 51, Loss: 0.1040, Train: 0.9082 AUC, Valid: 0.8160 AUC, Test: 0.7663 AUC\n",
      "---\n",
      "Run: 04, Epoch: 52, Loss: 0.1043, Train: 0.9012 AUC, Valid: 0.8142 AUC, Test: 0.7467 AUC\n",
      "---\n",
      "Run: 04, Epoch: 53, Loss: 0.1036, Train: 0.9047 AUC, Valid: 0.8184 AUC, Test: 0.7517 AUC\n",
      "---\n",
      "Run: 04, Epoch: 54, Loss: 0.1024, Train: 0.9034 AUC, Valid: 0.8206 AUC, Test: 0.7484 AUC\n",
      "---\n",
      "Run: 04, Epoch: 55, Loss: 0.1026, Train: 0.9078 AUC, Valid: 0.8083 AUC, Test: 0.7675 AUC\n",
      "---\n",
      "Run: 04, Epoch: 56, Loss: 0.1007, Train: 0.9095 AUC, Valid: 0.8188 AUC, Test: 0.7509 AUC\n",
      "---\n",
      "Run: 04, Epoch: 57, Loss: 0.1014, Train: 0.9144 AUC, Valid: 0.8283 AUC, Test: 0.7515 AUC\n",
      "---\n",
      "Run: 04, Epoch: 58, Loss: 0.1016, Train: 0.9158 AUC, Valid: 0.8210 AUC, Test: 0.7629 AUC\n",
      "---\n",
      "Run: 04, Epoch: 59, Loss: 0.1010, Train: 0.9129 AUC, Valid: 0.8108 AUC, Test: 0.7543 AUC\n",
      "---\n",
      "Run: 04, Epoch: 60, Loss: 0.0997, Train: 0.9155 AUC, Valid: 0.8109 AUC, Test: 0.7563 AUC\n",
      "---\n",
      "Run: 04, Epoch: 61, Loss: 0.0994, Train: 0.9135 AUC, Valid: 0.8313 AUC, Test: 0.7437 AUC\n",
      "---\n",
      "Run: 04, Epoch: 62, Loss: 0.0986, Train: 0.9172 AUC, Valid: 0.8098 AUC, Test: 0.7443 AUC\n",
      "---\n",
      "Run: 04, Epoch: 63, Loss: 0.0970, Train: 0.9229 AUC, Valid: 0.8139 AUC, Test: 0.7626 AUC\n",
      "---\n",
      "Run: 04, Epoch: 64, Loss: 0.0973, Train: 0.9205 AUC, Valid: 0.8031 AUC, Test: 0.7679 AUC\n",
      "---\n",
      "Run: 04, Epoch: 65, Loss: 0.0973, Train: 0.9246 AUC, Valid: 0.8144 AUC, Test: 0.7514 AUC\n",
      "---\n",
      "Run: 04, Epoch: 66, Loss: 0.0968, Train: 0.9205 AUC, Valid: 0.8226 AUC, Test: 0.7721 AUC\n",
      "---\n",
      "Run: 04, Epoch: 67, Loss: 0.0955, Train: 0.9243 AUC, Valid: 0.8248 AUC, Test: 0.7790 AUC\n",
      "---\n",
      "Run: 04, Epoch: 68, Loss: 0.0955, Train: 0.9204 AUC, Valid: 0.8309 AUC, Test: 0.7646 AUC\n",
      "---\n",
      "Run: 04, Epoch: 69, Loss: 0.0959, Train: 0.9160 AUC, Valid: 0.8280 AUC, Test: 0.7645 AUC\n",
      "---\n",
      "Run: 04, Epoch: 70, Loss: 0.0959, Train: 0.9286 AUC, Valid: 0.8196 AUC, Test: 0.7684 AUC\n",
      "---\n",
      "Run: 04, Epoch: 71, Loss: 0.0957, Train: 0.9284 AUC, Valid: 0.8164 AUC, Test: 0.7748 AUC\n",
      "---\n",
      "Run: 04, Epoch: 72, Loss: 0.0951, Train: 0.9254 AUC, Valid: 0.8286 AUC, Test: 0.7506 AUC\n",
      "---\n",
      "Run: 04, Epoch: 73, Loss: 0.0923, Train: 0.9339 AUC, Valid: 0.8267 AUC, Test: 0.7567 AUC\n",
      "---\n",
      "Run: 04, Epoch: 74, Loss: 0.0936, Train: 0.9296 AUC, Valid: 0.8170 AUC, Test: 0.7514 AUC\n",
      "---\n",
      "Run: 04, Epoch: 75, Loss: 0.0925, Train: 0.9339 AUC, Valid: 0.8146 AUC, Test: 0.7626 AUC\n",
      "---\n",
      "Run: 04, Epoch: 76, Loss: 0.0923, Train: 0.9319 AUC, Valid: 0.8277 AUC, Test: 0.7663 AUC\n",
      "---\n",
      "Run: 04, Epoch: 77, Loss: 0.0920, Train: 0.9349 AUC, Valid: 0.8178 AUC, Test: 0.7721 AUC\n",
      "---\n",
      "Run: 04, Epoch: 78, Loss: 0.0912, Train: 0.9392 AUC, Valid: 0.8130 AUC, Test: 0.7682 AUC\n",
      "---\n",
      "Run: 04, Epoch: 79, Loss: 0.0905, Train: 0.9386 AUC, Valid: 0.8165 AUC, Test: 0.7715 AUC\n",
      "---\n",
      "Run: 04, Epoch: 80, Loss: 0.0902, Train: 0.9342 AUC, Valid: 0.8093 AUC, Test: 0.7563 AUC\n",
      "---\n",
      "Run: 04, Epoch: 81, Loss: 0.0896, Train: 0.9392 AUC, Valid: 0.8187 AUC, Test: 0.7781 AUC\n",
      "---\n",
      "Run: 04, Epoch: 82, Loss: 0.0913, Train: 0.9373 AUC, Valid: 0.8102 AUC, Test: 0.7506 AUC\n",
      "---\n",
      "Run: 04, Epoch: 83, Loss: 0.0901, Train: 0.9412 AUC, Valid: 0.8053 AUC, Test: 0.7623 AUC\n",
      "---\n",
      "Run: 04, Epoch: 84, Loss: 0.0899, Train: 0.9362 AUC, Valid: 0.8060 AUC, Test: 0.7508 AUC\n",
      "---\n",
      "Run: 04, Epoch: 85, Loss: 0.0891, Train: 0.9414 AUC, Valid: 0.8115 AUC, Test: 0.7602 AUC\n",
      "---\n",
      "Run: 04, Epoch: 86, Loss: 0.0886, Train: 0.9397 AUC, Valid: 0.7845 AUC, Test: 0.7494 AUC\n",
      "---\n",
      "Run: 04, Epoch: 87, Loss: 0.0876, Train: 0.9408 AUC, Valid: 0.8105 AUC, Test: 0.7656 AUC\n",
      "---\n",
      "Run: 04, Epoch: 88, Loss: 0.0874, Train: 0.9423 AUC, Valid: 0.8216 AUC, Test: 0.7693 AUC\n",
      "---\n",
      "Run: 04, Epoch: 89, Loss: 0.0883, Train: 0.9442 AUC, Valid: 0.8117 AUC, Test: 0.7724 AUC\n",
      "---\n",
      "Run: 04, Epoch: 90, Loss: 0.0873, Train: 0.9479 AUC, Valid: 0.8122 AUC, Test: 0.7658 AUC\n",
      "---\n",
      "Run: 04, Epoch: 91, Loss: 0.0874, Train: 0.9455 AUC, Valid: 0.8123 AUC, Test: 0.7673 AUC\n",
      "---\n",
      "Run: 04, Epoch: 92, Loss: 0.0870, Train: 0.9451 AUC, Valid: 0.7961 AUC, Test: 0.7558 AUC\n",
      "---\n",
      "Run: 04, Epoch: 93, Loss: 0.0859, Train: 0.9463 AUC, Valid: 0.8158 AUC, Test: 0.7641 AUC\n",
      "---\n",
      "Run: 04, Epoch: 94, Loss: 0.0853, Train: 0.9402 AUC, Valid: 0.8077 AUC, Test: 0.7630 AUC\n",
      "---\n",
      "Run: 04, Epoch: 95, Loss: 0.0850, Train: 0.9447 AUC, Valid: 0.7773 AUC, Test: 0.7608 AUC\n",
      "---\n",
      "Run: 04, Epoch: 96, Loss: 0.0851, Train: 0.9476 AUC, Valid: 0.7944 AUC, Test: 0.7633 AUC\n",
      "---\n",
      "Run: 04, Epoch: 97, Loss: 0.0845, Train: 0.9528 AUC, Valid: 0.8087 AUC, Test: 0.7602 AUC\n",
      "---\n",
      "Run: 04, Epoch: 98, Loss: 0.0841, Train: 0.9530 AUC, Valid: 0.7771 AUC, Test: 0.7691 AUC\n",
      "---\n",
      "Run: 04, Epoch: 99, Loss: 0.0852, Train: 0.9541 AUC, Valid: 0.7841 AUC, Test: 0.7602 AUC\n",
      "---\n",
      "Run: 04, Epoch: 100, Loss: 0.0843, Train: 0.9575 AUC, Valid: 0.7905 AUC, Test: 0.7629 AUC\n",
      "---\n",
      "Run 04:\n",
      "Highest Train: 95.75\n",
      "Highest Valid: 83.40\n",
      "  Final Train: 89.69\n",
      "   Final Test: 75.83\n",
      "Run: 05, Epoch: 01, Loss: 0.1948, Train: 0.7279 AUC, Valid: 0.7034 AUC, Test: 0.6712 AUC\n",
      "---\n",
      "Run: 05, Epoch: 02, Loss: 0.1520, Train: 0.7513 AUC, Valid: 0.7568 AUC, Test: 0.6436 AUC\n",
      "---\n",
      "Run: 05, Epoch: 03, Loss: 0.1484, Train: 0.7170 AUC, Valid: 0.6562 AUC, Test: 0.6019 AUC\n",
      "---\n",
      "Run: 05, Epoch: 04, Loss: 0.1466, Train: 0.7472 AUC, Valid: 0.7282 AUC, Test: 0.7094 AUC\n",
      "---\n",
      "Run: 05, Epoch: 05, Loss: 0.1434, Train: 0.7629 AUC, Valid: 0.7749 AUC, Test: 0.7114 AUC\n",
      "---\n",
      "Run: 05, Epoch: 06, Loss: 0.1402, Train: 0.7701 AUC, Valid: 0.7785 AUC, Test: 0.7161 AUC\n",
      "---\n",
      "Run: 05, Epoch: 07, Loss: 0.1407, Train: 0.7773 AUC, Valid: 0.7433 AUC, Test: 0.7237 AUC\n",
      "---\n",
      "Run: 05, Epoch: 08, Loss: 0.1388, Train: 0.7851 AUC, Valid: 0.7835 AUC, Test: 0.6997 AUC\n",
      "---\n",
      "Run: 05, Epoch: 09, Loss: 0.1374, Train: 0.7947 AUC, Valid: 0.7820 AUC, Test: 0.7299 AUC\n",
      "---\n",
      "Run: 05, Epoch: 10, Loss: 0.1362, Train: 0.7866 AUC, Valid: 0.7705 AUC, Test: 0.7305 AUC\n",
      "---\n",
      "Run: 05, Epoch: 11, Loss: 0.1348, Train: 0.7946 AUC, Valid: 0.7474 AUC, Test: 0.7263 AUC\n",
      "---\n",
      "Run: 05, Epoch: 12, Loss: 0.1333, Train: 0.8052 AUC, Valid: 0.7434 AUC, Test: 0.7158 AUC\n",
      "---\n",
      "Run: 05, Epoch: 13, Loss: 0.1325, Train: 0.8047 AUC, Valid: 0.8050 AUC, Test: 0.7497 AUC\n",
      "---\n",
      "Run: 05, Epoch: 14, Loss: 0.1317, Train: 0.8160 AUC, Valid: 0.7908 AUC, Test: 0.7538 AUC\n",
      "---\n",
      "Run: 05, Epoch: 15, Loss: 0.1302, Train: 0.8288 AUC, Valid: 0.7941 AUC, Test: 0.7646 AUC\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "logger = repeat_experiments(\n",
    "    model, train_dataloader, val_dataloader, test_dataloader, \n",
    "    device, train_args, N_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "5e47ded3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All runs:\n",
      "Highest Train: 95.68  0.34\n",
      "Highest Valid: 82.48  1.06\n",
      "  Final Train: 87.76  5.19\n",
      "   Final Test: 75.44  2.03\n"
     ]
    }
   ],
   "source": [
    "# Final performance\n",
    "logger.print_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "df4f3ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This comparison only makes sense if using ~100 epochs during training\n",
    "norm_plot([\n",
    "    (test_auc_lb, test_auc_lb_std, \"GIN L.B.\"),\n",
    "    (0.7544, 0.0203, 'GIN local'),\n",
    "], 'Test ROC AUC Comparison over 10 runs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950defe6",
   "metadata": {},
   "source": [
    "In my experiments, I observe an average performance that approximately matches the leaderboard's, but with a standard deviation that's about 50% larger.  \n",
    "\n",
    "# Improvements with Virtual Nodes\n",
    "\n",
    "One architecture change that's common to see on the leaderboard for graph classification tasks are the inclusion of \"virtual nodes\", which seem to be first described in the [Neural message passing for quantum chemistry](https://arxiv.org/abs/1704.01212) paper.  For a given graph in the batch, the idea is to add a new node that shares a connection with all other nodes.  This creates an information pathway during message passing for node pairs that might be large distances from one another in the original graph.  If long-range information is important, which is likely to be the case with molecules modeled as graphs, then this may provide an alternative to e.g., building deep GNNs.\n",
    "\n",
    "One way to implement virtual nodes might be to modify each graph to add a new node that's fully connected, which would allow us to leverage built-in message passing functionality.  This is complicated, however, because these virtual nodes/edges do not have features like the normal nodes and edges in the graphs.  We choose here to not modify the topology of the underlying graphs and instead implement the message passing operations manually.  This will consist of two separate steps: 1) add the messages from the virtual nodes to each node in the graphs and 2) update the virtual nodes by adding in messages from all the other nodes in the graph.  \n",
    "\n",
    "## Messages *from* the virtual nodes\n",
    "\n",
    "Adding messages from the virtual nodes is made more complicated because we need to keep the computations separate for each component graph in the \"super-graph\".  We don't want to send messages from one graph in the batch to another, for example.  One other nuance is that we'll use a _shared virtual node embedding_ that represents the input features of the virtual node and is the same for all graphs in the dataset.  We will duplicate these virtual node parameters so every graph in the batch has its own copy.  \n",
    "\n",
    "When we send messages to the nodes we will need to convert from a graph-level tensor, where each row corresponds to a graph in the minibatch, to a nodel-level one.  To do this we will duplicate each graph's copy of the virtual node embedding for each of its nodes.  As an example, if we have two graphs in the batch which have 3 and 4 nodes, respectively, we'll first get: `h_virt_graph = [h_virt_g1, h_virt_g2]`, which creates two copies of the embedding for each graph in the batch.  We then transform that into a node-level representation like: `h_virt_node = [h_virt_g1, h_virt_g1, h_virt_g1, h_virt_g2, h_virt_g2, h_virt_g2, h_virt_g2]`, where each graph's copy is duplicated for every node it contains.  \n",
    "\n",
    "The reason we do this is it allows us to easily add the message from the correct virtual node to each of the nodes in the \"super-graph\".  Consider the following.  Let's say we have calculated the \"normal\" message passing result, where we have ignored the idea of virtual nodes:  `h_mp_normal = g.update_all(fn.copy('x', 'm'), fn.sum('m', 'h'))`.  This result would have a row for every node where the value represents the sum of all the messages in that node's neighborhood.  We can then simply add the message of the virtual node corresponding to the correct graph by: `h_mp_total = h_mp_normal + h_virt_node`, since `h_virt_node` has already taken care of duplicating the correct graph-level copy of the virtual node.  This will give the same result as if each graph had a copy of the virtual node in its graph structure during message passing.\n",
    "\n",
    "## Messages *to* the virtual nodes\n",
    "\n",
    "To update the representations of the virtual nodes throughout the GNN layers we also need to send messages from the nodes in the graphs back to the virtual nodes.  This is the reason each graph needs its own copy of the virtual node embedding, as each update to a virtual node should only contain messages from a single component graph.  We will implement this by operating on `h_virt_graph`, which has the graph-level copies of the virtual node embedding.  We will update these embeddings for a given GNN layer by:\n",
    "\n",
    "1. Summing the current representations of all the nodes in a component graph\n",
    "2. Adding that result to the graph's virtual node embedding\n",
    "3. Passing that result through an MLP that's shared across all virtual nodes\n",
    "\n",
    "Steps 1-2 represent the message passing operation for the virtual nodes.  The only challenge here is in collapsing the tensor of node representations back into graph-level ones, since we only want to sum the node embeddings coming from the same component graph.  We can use DGL's built-in [read-out](https://docs.dgl.ai/api/python/dgl.html#batching-and-reading-out-ops) functions for managing this complexity (e.g., `dgl.sum_nodes`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "bb4b030c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeGNNVirtual(nn.Module):\n",
    "    def __init__(self, emb_dim, num_layers, dropout):\n",
    "        \n",
    "        super(NodeGNNVirtual, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.convs = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        self.virtualnode_mlps = nn.ModuleList()\n",
    "        \n",
    "        self.atom_encoder = AtomEncoder(emb_dim)\n",
    "        self.bond_encoder = BondEncoder(emb_dim)\n",
    "        self.virtualnode_embed = nn.Embedding(1, emb_dim)\n",
    "        \n",
    "        \n",
    "        for layer in range(num_layers):\n",
    "            self.convs.append(GINLayer(emb_dim))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(emb_dim))\n",
    "            \n",
    "            if layer < self.num_layers-1:\n",
    "                self.virtualnode_mlps.append(nn.Sequential(\n",
    "                    nn.Linear(emb_dim, 2*emb_dim), \n",
    "                    nn.BatchNorm1d(2*emb_dim), \n",
    "                    nn.ReLU(), \n",
    "                    nn.Linear(2*emb_dim, emb_dim), \n",
    "                    nn.BatchNorm1d(emb_dim), \n",
    "                    nn.ReLU()\n",
    "                ))\n",
    "            \n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        # Atom embeddings\n",
    "        for emb in self.atom_encoder.atom_embedding_list:\n",
    "            nn.init.xavier_uniform_(emb.weight.data)\n",
    "        # Bond embeddings\n",
    "        for emb in self.bond_encoder.bond_embedding_list:\n",
    "            nn.init.xavier_uniform_(emb.weight.data)\n",
    "        # Virtual node embedding\n",
    "        nn.init.constant_(self.virtualnode_embed.weight, 0)\n",
    "        \n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        \n",
    "        for bn in self.batch_norms:\n",
    "            bn.reset_parameters()\n",
    "    \n",
    "    def forward(self, g):\n",
    "        h = self.atom_encoder(g.ndata['feat'])\n",
    "        edge_embedding = self.bond_encoder(g.edata['feat'])\n",
    "        \n",
    "        # Get a copy of the virtual node embedding for each graph in the batch\n",
    "        h_virt_graph = self.virtualnode_embed(\n",
    "            th.zeros(g.batch_size).long().to(h.device)\n",
    "        )\n",
    "        \n",
    "        \"\"\"\n",
    "        To add the message of the virtual node to every other node in the graph, we \n",
    "        need to duplicate each graph's copy of the virtual node embedding for every node \n",
    "        in that graph.  To do this, get an indicator for which graph each node belongs to.\n",
    "        E.g., if batch has 2 graphs and number of nodes for each graph is [2, 3], \n",
    "        get something like node_idx = [0, 0, 1, 1, 1].\n",
    "        \n",
    "        This will allow us to duplicate the graph-level copies of the virtual node\n",
    "        embedding, h_virt_graph, via: \n",
    "            h_virt_node = h_virt_graph[node_idx]\n",
    "        \n",
    "        The `dgl.broadcast_nodes` function may be helpful\n",
    "        \"\"\"\n",
    "        # TODO: get the node_idx tensor\n",
    "        # Beware: if using GPU, might need to call \".to(h.device)\" on \n",
    "        # tensors to move them to the correct device\n",
    "        node_idx = None\n",
    "        \n",
    "        for layer in range(self.num_layers):\n",
    "            # Convert from graph-level to node-level by making duplicates \n",
    "            # for each node in each graph\n",
    "            h_virt_node = h_virt_graph[node_idx]\n",
    "            \n",
    "            \n",
    "            # TODO: loop through the GIN and BatchNorm layers using same flow\n",
    "            # as NodeGNN (no ReLU on last layer), but include h_virt_node as \n",
    "            # an argument to the GIN layers.\n",
    "            \n",
    "                \n",
    "            # Update virtual node for next layer\n",
    "            if layer < self.num_layers - 1:\n",
    "                \n",
    "                with g.local_scope():\n",
    "                    # TODO: message passing (with self loop) for virtual nodes\n",
    "                    # Use a sum readout op to add a graph's node embeddings to h_virt_graph\n",
    "                    # E.g., virtual_mp = h_virt_graph + sum(h_j in G)\n",
    "                    virtual_mp = None\n",
    "                \n",
    "                # transform representation with MLP\n",
    "                h_virt_graph = F.dropout(\n",
    "                    self.virtualnode_mlps[layer](virtual_mp), \n",
    "                    self.dropout, \n",
    "                    training = self.training\n",
    "                )\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "e7ec3dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NodeGNNVirtual(\n",
       "  (convs): ModuleList(\n",
       "    (0): GINLayer(\n",
       "      (eps): nn.Parameter\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "        (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Linear(in_features=600, out_features=300, bias=True)\n",
       "      )\n",
       "      (bond_encoder): BondEncoder(\n",
       "        (bond_embedding_list): ModuleList(\n",
       "          (0): Embedding(5, 300)\n",
       "          (1): Embedding(6, 300)\n",
       "          (2): Embedding(2, 300)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): GINLayer(\n",
       "      (eps): nn.Parameter\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "        (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Linear(in_features=600, out_features=300, bias=True)\n",
       "      )\n",
       "      (bond_encoder): BondEncoder(\n",
       "        (bond_embedding_list): ModuleList(\n",
       "          (0): Embedding(5, 300)\n",
       "          (1): Embedding(6, 300)\n",
       "          (2): Embedding(2, 300)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): GINLayer(\n",
       "      (eps): nn.Parameter\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "        (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Linear(in_features=600, out_features=300, bias=True)\n",
       "      )\n",
       "      (bond_encoder): BondEncoder(\n",
       "        (bond_embedding_list): ModuleList(\n",
       "          (0): Embedding(5, 300)\n",
       "          (1): Embedding(6, 300)\n",
       "          (2): Embedding(2, 300)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): GINLayer(\n",
       "      (eps): nn.Parameter\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "        (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Linear(in_features=600, out_features=300, bias=True)\n",
       "      )\n",
       "      (bond_encoder): BondEncoder(\n",
       "        (bond_embedding_list): ModuleList(\n",
       "          (0): Embedding(5, 300)\n",
       "          (1): Embedding(6, 300)\n",
       "          (2): Embedding(2, 300)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): GINLayer(\n",
       "      (eps): nn.Parameter\n",
       "      (mlp): Sequential(\n",
       "        (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "        (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Linear(in_features=600, out_features=300, bias=True)\n",
       "      )\n",
       "      (bond_encoder): BondEncoder(\n",
       "        (bond_embedding_list): ModuleList(\n",
       "          (0): Embedding(5, 300)\n",
       "          (1): Embedding(6, 300)\n",
       "          (2): Embedding(2, 300)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (batch_norms): ModuleList(\n",
       "    (0): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (virtualnode_mlps): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "      (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Linear(in_features=600, out_features=300, bias=True)\n",
       "      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "      (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Linear(in_features=600, out_features=300, bias=True)\n",
       "      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "      (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Linear(in_features=600, out_features=300, bias=True)\n",
       "      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Linear(in_features=300, out_features=600, bias=True)\n",
       "      (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Linear(in_features=600, out_features=300, bias=True)\n",
       "      (4): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (atom_encoder): AtomEncoder(\n",
       "    (atom_embedding_list): ModuleList(\n",
       "      (0): Embedding(119, 300)\n",
       "      (1): Embedding(4, 300)\n",
       "      (2): Embedding(12, 300)\n",
       "      (3): Embedding(12, 300)\n",
       "      (4): Embedding(10, 300)\n",
       "      (5): Embedding(6, 300)\n",
       "      (6): Embedding(6, 300)\n",
       "      (7): Embedding(2, 300)\n",
       "      (8): Embedding(2, 300)\n",
       "    )\n",
       "  )\n",
       "  (virtualnode_embed): Embedding(1, 300)\n",
       ")"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vn = NodeGNNVirtual(emb_dim, num_layers, dropout)\n",
    "vn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2a5652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This forward pass should return a tensor without error\n",
    "vn(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "0c9d1d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vn = GraphGNN(emb_dim, num_layers, NodeGNNVirtual, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "616a688e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sum(p.numel() for p in model_vn.parameters()) == 3320706, \"Number of GIN + VN parameters doesn't match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6a4676",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 01, Epoch: 76, Loss: 0.0780, Train: 0.9762 AUC, Valid: 0.8198 AUC, Test: 0.7631 AUC\n",
      "---\n",
      "Run: 01, Epoch: 77, Loss: 0.0780, Train: 0.9746 AUC, Valid: 0.8023 AUC, Test: 0.7538 AUC\n",
      "---\n",
      "Run: 01, Epoch: 78, Loss: 0.0755, Train: 0.9762 AUC, Valid: 0.8363 AUC, Test: 0.7786 AUC\n",
      "---\n",
      "Run: 01, Epoch: 79, Loss: 0.0770, Train: 0.9795 AUC, Valid: 0.8295 AUC, Test: 0.7950 AUC\n",
      "---\n",
      "Run: 01, Epoch: 80, Loss: 0.0765, Train: 0.9794 AUC, Valid: 0.8417 AUC, Test: 0.8029 AUC\n",
      "---\n",
      "Run: 01, Epoch: 81, Loss: 0.0742, Train: 0.9818 AUC, Valid: 0.8323 AUC, Test: 0.7867 AUC\n",
      "---\n",
      "Run: 01, Epoch: 82, Loss: 0.0748, Train: 0.9818 AUC, Valid: 0.8247 AUC, Test: 0.7856 AUC\n",
      "---\n",
      "Run: 01, Epoch: 83, Loss: 0.0755, Train: 0.9825 AUC, Valid: 0.8287 AUC, Test: 0.7788 AUC\n",
      "---\n",
      "Run: 01, Epoch: 84, Loss: 0.0729, Train: 0.9844 AUC, Valid: 0.8274 AUC, Test: 0.8024 AUC\n",
      "---\n",
      "Run: 01, Epoch: 85, Loss: 0.0739, Train: 0.9806 AUC, Valid: 0.8322 AUC, Test: 0.7845 AUC\n",
      "---\n",
      "Run: 01, Epoch: 86, Loss: 0.0720, Train: 0.9840 AUC, Valid: 0.8098 AUC, Test: 0.7813 AUC\n",
      "---\n",
      "Run: 03, Epoch: 60, Loss: 0.0932, Train: 0.9401 AUC, Valid: 0.8115 AUC, Test: 0.7537 AUC\n",
      "---\n",
      "Run: 03, Epoch: 61, Loss: 0.0926, Train: 0.9372 AUC, Valid: 0.8133 AUC, Test: 0.7402 AUC\n",
      "---\n",
      "Run: 03, Epoch: 62, Loss: 0.0925, Train: 0.9460 AUC, Valid: 0.8065 AUC, Test: 0.7545 AUC\n",
      "---\n",
      "Run: 03, Epoch: 63, Loss: 0.0912, Train: 0.9376 AUC, Valid: 0.7995 AUC, Test: 0.7691 AUC\n",
      "---\n",
      "Run: 03, Epoch: 64, Loss: 0.0913, Train: 0.9439 AUC, Valid: 0.7833 AUC, Test: 0.7698 AUC\n",
      "---\n",
      "Run: 03, Epoch: 65, Loss: 0.0899, Train: 0.9479 AUC, Valid: 0.8239 AUC, Test: 0.7681 AUC\n",
      "---\n",
      "Run: 03, Epoch: 66, Loss: 0.0902, Train: 0.9501 AUC, Valid: 0.8116 AUC, Test: 0.7533 AUC\n",
      "---\n",
      "Run: 03, Epoch: 67, Loss: 0.0903, Train: 0.9432 AUC, Valid: 0.8011 AUC, Test: 0.7632 AUC\n",
      "---\n",
      "Run: 03, Epoch: 68, Loss: 0.0891, Train: 0.9483 AUC, Valid: 0.7851 AUC, Test: 0.7576 AUC\n",
      "---\n",
      "Run: 03, Epoch: 69, Loss: 0.0887, Train: 0.9524 AUC, Valid: 0.8011 AUC, Test: 0.7510 AUC\n",
      "---\n",
      "Run: 03, Epoch: 70, Loss: 0.0886, Train: 0.9573 AUC, Valid: 0.7943 AUC, Test: 0.7572 AUC\n",
      "---\n",
      "Run: 03, Epoch: 71, Loss: 0.0874, Train: 0.9555 AUC, Valid: 0.7768 AUC, Test: 0.7544 AUC\n",
      "---\n",
      "Run: 03, Epoch: 72, Loss: 0.0860, Train: 0.9576 AUC, Valid: 0.7992 AUC, Test: 0.7652 AUC\n",
      "---\n",
      "Run: 03, Epoch: 73, Loss: 0.0858, Train: 0.9590 AUC, Valid: 0.7908 AUC, Test: 0.7361 AUC\n",
      "---\n",
      "Run: 03, Epoch: 74, Loss: 0.0862, Train: 0.9589 AUC, Valid: 0.7633 AUC, Test: 0.7542 AUC\n",
      "---\n",
      "Run: 03, Epoch: 75, Loss: 0.0859, Train: 0.9538 AUC, Valid: 0.7891 AUC, Test: 0.7335 AUC\n",
      "---\n",
      "Run: 03, Epoch: 76, Loss: 0.0865, Train: 0.9631 AUC, Valid: 0.7956 AUC, Test: 0.7623 AUC\n",
      "---\n",
      "Run: 03, Epoch: 77, Loss: 0.0851, Train: 0.9577 AUC, Valid: 0.7635 AUC, Test: 0.7300 AUC\n",
      "---\n",
      "Run: 03, Epoch: 78, Loss: 0.0859, Train: 0.9574 AUC, Valid: 0.7678 AUC, Test: 0.6859 AUC\n",
      "---\n",
      "Run: 03, Epoch: 79, Loss: 0.0836, Train: 0.9658 AUC, Valid: 0.7694 AUC, Test: 0.7558 AUC\n",
      "---\n",
      "Run: 03, Epoch: 87, Loss: 0.0812, Train: 0.9706 AUC, Valid: 0.8074 AUC, Test: 0.7257 AUC\n",
      "---\n",
      "Run: 03, Epoch: 88, Loss: 0.0799, Train: 0.9745 AUC, Valid: 0.7585 AUC, Test: 0.7569 AUC\n",
      "---\n",
      "Run: 03, Epoch: 89, Loss: 0.0785, Train: 0.9713 AUC, Valid: 0.7941 AUC, Test: 0.7578 AUC\n",
      "---\n",
      "Run: 03, Epoch: 90, Loss: 0.0790, Train: 0.9761 AUC, Valid: 0.8067 AUC, Test: 0.7314 AUC\n",
      "---\n",
      "Run: 03, Epoch: 91, Loss: 0.0782, Train: 0.9741 AUC, Valid: 0.8096 AUC, Test: 0.7582 AUC\n",
      "---\n",
      "Run: 03, Epoch: 92, Loss: 0.0796, Train: 0.9765 AUC, Valid: 0.8056 AUC, Test: 0.7539 AUC\n",
      "---\n",
      "Run: 03, Epoch: 93, Loss: 0.0767, Train: 0.9740 AUC, Valid: 0.7918 AUC, Test: 0.7392 AUC\n",
      "---\n",
      "Run: 03, Epoch: 94, Loss: 0.0779, Train: 0.9789 AUC, Valid: 0.8135 AUC, Test: 0.7635 AUC\n",
      "---\n",
      "Run: 03, Epoch: 95, Loss: 0.0763, Train: 0.9760 AUC, Valid: 0.7887 AUC, Test: 0.7544 AUC\n",
      "---\n",
      "Run: 03, Epoch: 96, Loss: 0.0772, Train: 0.9792 AUC, Valid: 0.7924 AUC, Test: 0.7671 AUC\n",
      "---\n",
      "Run: 03, Epoch: 97, Loss: 0.0756, Train: 0.9757 AUC, Valid: 0.7867 AUC, Test: 0.7388 AUC\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "loggers_vn = repeat_experiments(\n",
    "    model_vn, train_dataloader, val_dataloader, test_dataloader, \n",
    "    device, train_args, N_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "872b9e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All runs:\n",
      "Highest Train: 98.50  0.38\n",
      "Highest Valid: 83.69  0.77\n",
      "  Final Train: 91.14  3.59\n",
      "   Final Test: 76.81  1.63\n"
     ]
    }
   ],
   "source": [
    "# Final performance\n",
    "loggers_vn.print_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "4f13ee83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This comparison only makes sense if using ~100 epochs during training\n",
    "norm_plot([\n",
    "    (test_auc_lb, test_auc_lb_std, \"GIN L.B.\"),\n",
    "    (0.7544, 0.0203, 'GIN local'),\n",
    "    (0.7681, 0.0163, 'GIN w/ Virtual Nodes'),\n",
    "], 'Test ROC AUC Comparison over 10 runs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49c0d76",
   "metadata": {},
   "source": [
    "My experiments show a 2% relative performance improvement and a similar variance compared to the experiments without virtual nodes.  Although this is in line with the Leaderboard, these numbers do not provide a statistically significant result showing an improvement over the baseline.\n",
    "\n",
    "# Extra Credit\n",
    "In general, it's a great use of time to find a submission on the leaderboard that's interesting and figure out how to implement it.  Here are two possible avenues for additional exploration:\n",
    "\n",
    "1. Graph Norm is a method for normalizing node representations for each graph and is designed as a GNN-specific normalization method to help convergence and generalization.  Here are the [paper](https://arxiv.org/abs/2009.03294) and [code](https://github.com/lsj2408/GraphNorm/blob/master/GraphNorm_ws/ogbg_ws/src/dgl_model/norm.py).\n",
    "2. Free Large-scale Adversarial Augmentation on Graphs (FLAG) is a method for adding adversarial perturbations to the training node features to improve generalization.  Here are the [paper](https://arxiv.org/abs/2010.09891) and [code](https://github.com/devnkong/FLAG). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08cd39a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
